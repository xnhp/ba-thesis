% Load the kaobook class
\documentclass[
	fontsize=10pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	secnumdepth=1, % How deep to number headings. Defaults to 1 (sections)
  toc=indentunnumbered % indent unnumbered entries in toc. if not used, they
                       % will be on the same indent as numbered parent entries
]{kaobook}

% Choose the language
\usepackage[english]{babel} % Load characters and hyphenation
\usepackage[english=british]{csquotes}	% English quotes

% === CUSTOM THINGS ===
\usepackage{mycommands}
\usepackage{nicefrac}
\usepackage{commath}  % for \abs and \norm, cf https://tex.stackexchange.com/a/149022
\usepackage{chemformula}
\usepackage{optidef}  % typesetting optimisation problems
\usepackage{hyperref}
\usepackage{listings}

\usepackage{amssymb} % for checkmarks

% Load the bibliography package
\usepackage[style=numeric,sorting=none]{kaobiblio}  
% \usepackage[style=numeric,sorting=none]{biblatex}
\addbibresource{./BA.bib}

\usepackage{subcaption}

% algorithm typesetting setup
\usepackage[linesnumbered,ruled]{algorithm2e}
\setlength{\algoheightrule}{0pt}
\setlength{\algotitleheightrule}{0pt}
% \SetCommentSty{\familydefault}
\newcommand\mycommfont[1]{\footnotesize \textcolor{teal}{#1}}
\SetCommentSty{mycommfont}

% ======

% Load packages for testing
\usepackage{blindtext}
%\usepackage{showframe} % Uncomment to show boxes around the text area, margin, header and footer
%\usepackage{showlabels} % Uncomment to output the content of \label commands to the document where they are used
  

% Load mathematical packages for theorems and related environments
\usepackage{kaotheorems}

% Load the package for hyperreferences
\usepackage{kaorefs}

\graphicspath{{images/}{./}{../GraphGym/run/}} % Paths where images are looked for

\makeindex[columns=3, title=Alphabetical Index, intoc] % Make LaTeX produce the files required to compile the index


\begin{document}

%----------------------------------------------------------------------------------------
%	BOOK INFORMATION
%----------------------------------------------------------------------------------------

% \titlehead{Document Template}
\title[Node Duplication in Disease Maps using Graph Neural Networks]{Node
  Duplication in Disease Maps using Graph Neural Networks}
\author[BM]{Benjamin Moser}
\date{\today}
% \publishers{An Awesome Publisher}

%----------------------------------------------------------------------------------------

\frontmatter % Denotes the start of the pre-document content, uses roman numerals

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\makeatletter
\uppertitleback{\@titlehead} % Header

\lowertitleback{
	\textbf{Disclaimer} \\
	You can edit this page to suit your needs. For instance, here we have a no copyright statement, a colophon and some other information. This page is based on the corresponding page of Ken Arroyo Ohori's thesis, with minimal changes.
	
	\medskip
	
	\textbf{No copyright} \\
	\cczero\ This book is released into the public domain using the CC0 code. To the extent possible under law, I waive all copyright and related or neighbouring rights to this work.
	
	To view a copy of the CC0 code, visit: \\\url{http://creativecommons.org/publicdomain/zero/1.0/}
	
	\medskip
	
	\textbf{Colophon} \\
	This document was typeset with the help of \href{https://sourceforge.net/projects/koma-script/}{\KOMAScript} and \href{https://www.latex-project.org/}{\LaTeX} using the \href{https://github.com/fmarotta/kaobook/}{kaobook} class.
	
	\medskip
	
	\textbf{Publisher} \\
	First printed in May 2019 by \@publishers
}
\makeatother

%----------------------------------------------------------------------------------------
%	DEDICATION
%----------------------------------------------------------------------------------------

% \dedication{
% 	The harmony of the world is made manifest in Form and Number, and the heart and soul and all the poetry of Natural Philosophy are embodied in the concept of mathematical beauty.\\
% 	\flushright -- D'Arcy Wentworth Thompson
% }

%----------------------------------------------------------------------------------------
%	OUTPUT TITLE PAGE AND PREVIOUS
%----------------------------------------------------------------------------------------

% Note that \maketitle outputs the pages before here
\maketitle

%----------------------------------------------------------------------------------------
%	PREFACE
%----------------------------------------------------------------------------------------

\chapter*{Abstract}


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LIST OF FIGURES/TABLES
%----------------------------------------------------------------------------------------

\begingroup % Local scope for the following commands

% Define the style for the TOC, LOF, and LOT
%\setstretch{1} % Uncomment to modify line spacing in the ToC
%\hypersetup{linkcolor=blue} % Uncomment to set the colour of links in the ToC
\setlength{\textheight}{230\vscale} % Manually adjust the height of the ToC pages

% Turn on compatibility mode for the etoc package
\etocstandarddisplaystyle % "toc display" as if etoc was not loaded
\etocstandardlines % "toc lines as if etoc was not loaded

\tableofcontents % Output the table of contents

% TODO include?
% if so, set up short captions.
\listoffigures % Output the list of figures

% Comment both of the following lines to have the LOF and the LOT on different pages
\let\cleardoublepage\bigskip
\let\clearpage\bigskip

% TODO include? probably not.
\listoftables % Output the list of tables

\endgroup

%----------------------------------------------------------------------------------------
%	MAIN BODY
%----------------------------------------------------------------------------------------

\mainmatter % Denotes the start of the main document content, resets page numbering and uses arabic numbers

% numbering for subsections in TOC? https://tex.stackexchange.com/a/418123
% will result in indentation
% but this also increases numberings. can use this as a fallback if we dont
% figure it out otherwise.
% \setcounter{secnumdepth}{3}

\setchapterstyle{plain} % Choose the default chapter heading style

\pagelayout{wide} % No margins




\chapter{Introduction}
% see [[Writing Outline]]

% have to deal with duplicates due to some species appearing often
% and potentially involved in crosstalk

% duplicates are problem because (rephrase from what i wrote in background)

% tools already do support displaying or introducing duplicates

% but decision of duplication still up to the curator

% curating layout is insanely labour-intensive process

% distinction of use-cases (identify *all*, or few high-confidence suggestions)?
% how important is this even?


\paragraph{Overview} ...
% bit of background, where are we, what is the problem

\paragraph{Problem Statement} ...
% what is the concrete problem we are tying to solve?

\paragraph{Motivation for Approach} ...
% what do we want to contribute on top of this?

% (fragment)
% As such, it is problematic to hinge the decision of duplication on the identitiy
% of the node alone, since the same species may be a connector in some cases and
% not in other cases. As elaborated above, we have to consider its neighbourhood.

% (fragment)
% Motivated by this, we seek to implicitly capture the criteria for node
% duplication by training a machine learning model on a sequence of snapshots of a
% diagram during a reorganisation process performed by an expert. The model is
% trained to predict whether a node should be duplicated or not.

% (fragment)
% Moreover, finding the exact number of duplicates to introduce and how to connect
% them is not trivial and was commonly left to human curators. We provide a basic
% approach to answer this.

% (fragment)
% Further, it remains an open to question where to position the newly introduced
% duplicates in the layout. We leave this question to future work.

% (fragment)
% While structural features such as an extremely large node degree
% immediately come to mind, such simple predictors often do not suffice to capture
% all cases (see \nameref{sec:experiments-results}).
% % TODO elaborate more?

\paragraph{Structure} ...



% TODO some prose on how biological systems are complex, inspired by
% https://www.quantamagazine.org/biologists-rethink-the-logic-behind-cells-molecular-signals-20210916/

% motivation for disease maps
% Our understanding of the molecular mechanisms that are involved in biological
% systems is improving drastically. However, this knowledge is growing
% incrementally and is often scattered across individual scientific publications.

% To properly consider complex signalling effects such as
% activation/inhibition, crosstalk or feedback, it is of the essence to make this
% network structure accessible to both human cognition and computational analysis
% \ cite{barabasi_NetworkBiologyUnderstanding_2004}.

% TODO node duplication important in
% - general network analysis, as preprocessing (manipur, iPAO1, ...)
% - visualisation

% may not be a good idea to simply duplicate anything e.g. involved in more than
% one pathway because of *key connectors*

% becomes particularly relevant now that we are layouting more than single
% pathways, general connectivity is importnat

% overview (this work is structured as follows blah)
% basic concepts given in background.
% mention that abbreviations etc are given in appendix
% position against other work in related work
% descibe approach in methods
% give experiments and results

\chapter{Background}
% TODO chapter overview

\section{Biological Networks}
% NOTE already did one pass of editing

The behaviour of biological systems is often shaped by complex interactions.
% potentially across different levels of abstraction.
% In the most general sense, we refer to biological entities as \textit{species}.
%
% dont really need to list possible types if we give concrete examples below
% Possible species types are
% proteins, genes, but possibly also phenotype descriptions or drugs.
% %
% Possible relations include chemical reactions such as state transitions (e.g.
% phosphorylation), physical interactions between two proteins or the effect a drug
% has on the function of some protein.
%
%
The set of entities in a system and their interactions (relationships) naturally
form a network. We refer to a biological entity in such an interaction network
as \ild{species}. Choices of what species and relationships to consider yield
different kinds of biological networks.

\ild{Protein-protein interaction} (PPI) networks describe the complex
interactions between different proteins. Analysing the entire network of
interactions is interesting because the effective biological function of
proteins is rarely defined based on their identity alone, but rather on their
roles as enzymes or signalling molecules in relationship to other proteins.
\textsc{PPI} networks can be useful to infer the biological function of an
unknown protein or gene, or to group functionally similar proteins.
% TODO more cites -- e.g. the thing from the context?

A \ild{metabolic model} of some organism consists of a formally described set of
chemical compounds (\ild{metabolites}) as well as a set of chemical reactions or
interactions between the metabolites. Computational analysis methods on
metabolic networks can be used to predict the growth of an organism under
specific conditions, identify key intervention targets to alter metabolic
processes, or identify relatively independent metabolic subsystems.
% TODO citations

\ild{Disease maps} visually describe species and interactions that are of relevance
to a specific disease. They serve as a comprehensive and coherent collection of
existing knowledge that is otherwise scattered across individual publications.

Species and relationships need not necessarily have a direct physical
counterpart. Several works investigate the interactions between diseases, drugs,
or phenotypes, potentially connecting concrete physical entities (such as
proteins) to abstract entities (such as, for instance, drug side effects)
\cite{ruiz_identification_2021, barabasi_NetworkMedicineNetworkbased_2011}.
% TODO cite that work on gene classification for cancer thing presented by janina
% TODO cite more
Moreover, biological networks
% such as PPI networks or metabolic models
may serve
as a scaffold to put data on individual species into relation with one another.
% TODO example: manipur cancer classification, scRNA analysis
% maybe describe in one more sentence.
% to integrate data on, e.g., gene expression or reaction rates.
% This can be used to classify cancer
% TODO more precise and cite, the work I looked at for prop for whole-graph
% classification

\section{Disease Maps}

% what we want to say is: we already have (small) pathway diagrams, but they are
% not enough for describing a disease

Individual biochemical pathways can be described visually by \ild{process
  description diagrams}. Species are represented visually as discs or boxes and
are linked by lines representing chemical processes.
We refer to the visual representation of a species as a \ild{species alias}.
Such diagrams have traditionally been
used to describe reaction cascades and metabolic subsystems, first in the form
of hand-drawn illustrations and later as computer-generated graphics.
An example is given in
\reffig{process-diagram-old-vs-new}. 

However, several diseases such Alheimer's Disease or Parkinson's Disease do not depend
merely on a single mechanism. Rather, they are thought to arise through complex
interactions of biological processes or genetic and environmental factors.
Knowledge on relevant factors is obtained incrementally and scattered across
% TODO different word for relevant factors
individual publications or database entries.
Thus, a systematic understanding of the involved processes and their
relationships is essential
% it is of the essence to make this
% network structure accessible to both human cognition and computational analysis
% \ cite{barabasi_NetworkBiologyUnderstanding_2004}.
\cite{ostaszewski_CommunitydrivenRoadmapIntegrated_2019,
  mazein_SystemsMedicineDisease_2018}.
% 

For several diseases, experts have assembled
\ild{disease maps}, comprehensive visual diagrams combining all known mechanisms
relevant for a given disease.
These diagrams are particularly suited for visual, interactive exploration.
An example is given in
\reffig{process-diagram-old-vs-new}.

Traditionally, such diagrams have been drawn as pixel- or vector-based graphics.
Formalised, digital representations provide several advantages:
\begin{itemize}
  \item Creating a disease map requires a high amount of effort and domain
    knowledge. The extraction of knowledge from scientific publications or
    databases, as well as finding an adequate visual layout can be supported by
    computational tools.
\item Entities in the diagram may be annotated with additional information
  such as links to research publication or database entries.
\item A formalised representation enables the use of computational methods for
  analysis and interactive exploration (see \nameref{sec:related-work}).
\end{itemize}

Although their content is based on biological processes, disease maps differ in
nature from other types of biological networks in the following aspects:
\begin{itemize}
\item Disease maps are assembled based on the judgement of their curators. Only
  processes that are deemed relevant or informative to the given objective are
  included.
\item While other biological networks such as PPI networks or metabolic networks
  are defined mainly by their abstract network structure, a disease map is an
  actual visual diagram. Species and relationships have been laid out to
  optimally present the included information.
  % TODO maybe move this to 'drawing of ...'
% Although several approaches for the drawing of large process
%   diagrams exist (see TODO), it is still common practise
%   to invest manual effort into the layout.
\item
  Recent disease maps contain up to several thousands of species and reactions and
  are very rich in information beyond the mere enumeration of species and their
  pairwise relationships. For example, a disease map may contain different types
  of species such as proteins, genes, phenotypes \etc. Species may have
  different states (e.g. ``phosphorylated'') and be nested in \ild{complexes}
  (groups). Further, species and relationships are often
  annotated with links to external databases such as Entrez Gene
  \cite{maglott_EntrezGeneGenecentered_2005} or UniProt
  \cite{theuniprotconsortium_UniProtUniversalProtein_2021}.
  This poses special challenges particularly for layout and
  interactive exploration. 
\end{itemize}


% TODO (low) how are disease maps created?


% TODO vertical alignment of subfigure labels
\begin{figure}[h]
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{NF-kB-mechanism/handdrawn.png}
    \caption{Manually created diagram.}
    \label{fig:process-diagram-old-vs-new:handdrawn}
  \end{subfigure}
  \hspace{1em}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{NF-kB-mechanism/CellDesigner.png}
    \caption{Diagram as created with \celldesigner.}
    \label{fig:process-diagram-old-vs-new:celldesigner}
  \end{subfigure}
  \caption{
    Two representations of prototypical mechanisms of \nfkb-signaling.
    % TODO describe roughly what this is for.
    % TODO descibe pros/cons of each
    % TODO describe why this is a nice example for a'' `complex interaction'
  }
  \label{fig:process-diagram-old-vs-new}
\end{figure}
% TODO replace with image from  http://www.celldesigner.org/features.html ?

% TODO example image of a large disease map
\begin{figure}[h]
  \centering
  \pic{disease-map-screenshots/pdmap-excerpt.png}
  \caption[Excerpt of \PDMap (Parkinson's Disease)]{Excerpt of \PDMap
    \cite{fujita_IntegratingPathwaysParkinson_2014}, describing molecular
    mechanisms of Parkinson's Disease.}
  \label{fig:pdmap-example}
\end{figure}

% TODO relationships to other kinds of biological networks


% -------------------------------------
\section{Drawing of Biological Process Diagrams}
\label{sec:draw-biol-netw}
% mainly requirements, and some current research

The most widely used and intuitive visualisation paradigm is the
\textit{node-link diagram} in which nodes are represented graphically as dots,
circles or boxes and edges are represented by lines. For sake of simplicity, we
use \textit{nodes} and \textit{edges} ambiguously both in their mathematical
sense and for their graphical representations.
%
Finding a \textit{layout} of a graph (also called \textit{graph drawing}) means
finding positions for node representations and potentially also routing edges.
%
What makes a good layout generally depends on the specific kind of network data and the
task of the consumer of the visualisation.
%
In general, a good layout is commonly required to avoid edge crossings or
overlapping nodes, to be compact and to keep euclidean distances proportional to
graph distances.
% TODO citation here?
Additional constraints may be for example the preservation of symmetries, clear
representation of hierarchical structure or preservation of a viewer's mental map when
updating a dynamically changing graph layout.
%
Automatic graph layouting is a highly studied topic and numerous different
methods are available (see \refsec{related-work}).

Biological process diagrams, however, often contain subgraphs with particular
semantics, which must be considered explicitly in the layout in order to convey
the contained information as effectively as possible.
Because of this, it is still common practise to draw such diagrams manually,
or to obtain an initial automatic layout and adjust that manually. This process is
extremely time-consuming and often requires specific domain knowledge.
% TODO cite anecdote of PDMap? I think mentioned in one of the cites above.
We outline some of the challanges in drawing biological process diagrams
\cite{bourqui_MetabolicNetworkVisualization_2007,
  siebenhaller_HumanlikeLayoutAlgorithms_2020,
  wu_MetabopolisScalableNetwork_2019}:

% TODO clarify: talking about process diagrams here but applies as well do
% disease maps

\begin{itemize}
% \item Process diagrams may be created to convey or highlight particular
%   information and express an intentional message.
 \item Large diagrams such as the disease maps considered in this work often
   exhibit a clear structural and visual hierarchy.
 \item Biochemical reactions often involve main substrates and products as well
   as secondary cofactors and enzymes. Substrates and products are usually
   placed orthogonally on opposite sides while cofactors and enzymes are
   placed on the side.
 \item Biological pathways often involve reaction cascades. These should be
   displayed such that the cascade is distuingishable and easy to follow.
   It may be preferrable to align cascades to the natural reading direction of
   the viewer (commonly top-to-bottom and left-to-right).
 \item Some biological pathways involve cyclic patterns and these should be
   clearly distuingishable as such.
\item Biological process description diagrams may contain more information
  than merely species and reactions. Species can be (recursively) grouped
  into complexes. A complex is commonly represented as a box containing the
  visual representation of the species and thus its contents also need to be
  laid out. Further, cellular compartments are also commonly represented
  visually as large boxes or frames containing the biological processes
  therein. Since transport in and out of the compartment and other reactions
  involving the membranes are of high biological relevance, proper placement
  of nodes and edges inside, outside or on the boundary of compartments is critical.
  % already bit relevant to node duplication
\item It may be the case that a single species often is involved in several
  different biological processes. It may be preferrable to represent that
  species by multiple, separate visual representations.
\end{itemize}

The question of if and when to represent a species by multiple different visual
representations instead of a single one is exactly the focus of this work.


\section{Node Duplication \& Connectivity}
% TODO make this more thorough, particularly why this is a problem?
% TODO here should go, elaborately why node duplication is a problem

If the same species $S$ is involved in several different biological processes,
$S$ may either be represented by a single visual representation and linked to
all involved processes, or it may be represented by multiple visual
representations, each linked only to some processes.
%
In any case, each path through $S$ defines a connectivity
between the involved processes.

However, it may be the case that some processes involve the same
species, but that species' role is completely unrelated between the two
processes. This is the case, for instance, if the processes involve different
physical instances of that species, if the species is available in such
abundance that the actual physical instance is irrelevant, or if the species is
merely an unimportant byproduct.
% different physical identities
% potentially same, but irrelevant

Thus, if a species $S$ is involved in multiple processes, we need to assess
between which of these processes there exists in fact a \ild{true connectivity}
via $S$; or which merely involve $S$ in different, unrelated contexts
(\ild{false connectivity}).
%
True connectivity should be represented in the network structure and the visual
diagram as a path through $S$. There should be no edges implying false
connectivity.
False connectivity can be resolved by introducing another visual representation
for $S$ and re-attaching any incident edges. We refer to this procedure as
\ild{node duplication}.

These considerations are important for finding a faithful diagram layout.
If the number of involved processes is large, having only a single graphical
representation may lead to a high amount of visual noise in the diagram: there
may be a large number of edges, covering a long distance and linking actually
completely unrelated processes.
%
Having many independent representations
certainly makes it easier to avoid visual noise. However, connections between
different subgraphs may no longer be encoded explicitly, omitting crucial
information from the diagram.

The criteria for deciding between true and false connectivity, and thus node
duplication are not immediately clear. In practise, the decision is made by
experts case-by-case, or general heuristics are employed (see
\nameref{sec:related-work}). In this work, we aim for a more precise approach to
decide node duplication automatically in the context of disease maps.

% TODO: visual example



% -------------------------------------
\section{Biological Databases and Ontologies}
\label{sec:ontologies}

% (mainly introduce gene ontology)

% main motivation: research projects deal in principle with the same entities
% (e.g. all mean the same type of ATP molecule), but entities are usually still
% described in natural language (as strings). Same holds for relationships.

% this is problematic becaus string representations are ambiguous and tricky to
% process / compare / identify
Different research projects on the same organism or disease deal in principle
with the same universal sets of biological entities and relationships. For
instance, if two projects were to describe the metabolic network of \ecoli, both
are likely to mention the function of Adenosine Triphosphate (ATP), a basic
molecule that appears in many metabolic reactions. Both projects use the same
notion of ATP and its effects, yet they may describe it differently, e.g., by
its full name, abbreviation or
% TODO commas for e.g.?
chemical formula. This hinders knowledge transfer and -integration. We are
striving towards a structured, formalised body of knowledge for representing
information about physical entities such as molecules, but also on biological terms
describing processes (e.g. ``glycolysis''), localisation (e.g. ``cytoplasm'') or
functions.
% TODO not nice, maybe example for functions

There are several publicly available databases that gather information on
biological entities such as proteins (\toolname{UniProt}
\cite{theuniprotconsortium_UniProtUniversalProtein_2021}), genes
(\toolname{EntrezGene} \cite{maglott_EntrezGeneGenecentered_2005}) or drugs
(\toolname{DrugBank} \cite{wishart_DrugBankKnowledgebaseDrugs_2008}). These
databases often gather basic information and related research about the entity.
% maybe https://sci-hub.st/10.1002/0471250953.bi0101s50

Likewise, biological terms describing processes, functions or cellular
localisation can be represented in an \ild{ontology}, a (directed acyclic) graph
of terms and their relationships. For instance, the terms ``glycolysis'' and
``carbohydrate metabolic process'' may be connected by a \textit{involved-in}
relationship. The Gene Ontology project \cite{ashburner_GeneOntologyTool_2000}
provides three distinct ontology graphs describing molecular functions, cellular
component or biological processes. Each ontology graph loosely describes a
relationship, however, links between hierarchies may also exist, for instance
that the DNA repair process occurs in the Mitochondrion.
% TODO do need to make clearer what'' `function' stands for.




\section{Supervised Learning for Classification}
\label{sec:supervised-learning}
Our goal is to predict whether a species alias in a disease map
should be duplicated or not. For each species alias, we extract 
\textit{features}, which we deem to be characteristic for the target label of
the node (see \nameref{sec:feature-selection}).
% TODO check ref
Additionally, we are given training
data in the form of one or several disease maps and a binary label for each
species alias describing whether it was duplicated during manual curation. We
aim to use this training data to fit a machine learning model such that it will
be able to make meaningful predictions for species aliases in other disease
maps.

We are working in the setting of \ild{supervised learning}, which we
briefly introduce in the following
\cite{vapnik_PrinciplesRiskMinimization_,bronstein_geometric_2021}:
% TODO better-looking references?
We consider $n$ observations $\mathcal{T} = \{(\vec x_i, y_i)\}_{i=1}^n$ (also
% TODO this would be saying we evaluate on data we trained with, lets phrase
% this better.
called \ild{training data} or \ild{examples}) to be drawn from an (unknown)
distribution $P$ over $\mathcal{X} \times \mathcal{Y}$ where $\mathcal{Y}$ is
the label domain and $\mathcal{X}$ is the feature domain. For the scope of this
work, we restrict ourselves to the \ild{binary classification problem}, i.e.
$\mathcal{Y} = \{0,1\}$ . We assume that $\mathcal{X} = \mathbb{R}^d$ and the
number of dimensions $d$ is large. Let us denote the set of
observed \ild{ground-truth} labels as $\mat Y := \{y_i\}_{i=1}^n$ and the set of
observed feature vectors as $\mat X := \{x_i\}_{i=1}^n$. $\mat X$ can also be
seen as a matrix in $\mathbb{R}^{n \times d}$.
%
We assume that labels are generated by an unknown function $f$ such that $y_i
= f(\vec x_i)$.
%
Further, we are given a \ild{loss function} $\mathcal{L} :
\mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ that describes how
different a prediction is from the true outcome.
%
In general, we aim to find a (parameterised) function $\tilde f$ that minimises
the \ild{true risk} $\mathcal{R}(\tilde f) := \mathbb{E}_P\left[ \mathcal{L(y_i,
    \tilde f(\vec x_i))} \right]$, where $\mathbb{E}_P$ is the expected value
over $P$.
%
Since the original distribution $P$ is unknown, we instead seek to minimise the 
 \ild{empirical risk} on the training data
$\mathcal{R}_{\text{emp}}(\tilde f) := \nicefrac{1}{n}
\sum_{i=1}^n \mathcal{L}(y_i, \tilde f(\vec x_i))$ and assume the empirical risk
approximates the true risk.
%
In the following, we call a concrete choice of $\tilde f$ a \ild{model} or a
\ild{classifier}. $\tilde f$ is commonly parameterised by a set of \ild{model parameters}
$\theta$.


% TODO does this still apply? how does this relate to MLE as introduced via the
% BCE loss?


% -------------------------------------
\section{Graph Neural Networks}
% only give definitions here
% TODO refer to Related Work for actual research stuff
\label{sec:neural-networks}

% The terminology used is still developing. There are many ways in which neural
% networks may exploit graph structure \footnote{
%   For instance, we do not consider here the mechanism of \ild{pooling} through
%   which the input graph can be coarsened w.r.t to its latent features.
% \cite{ying_hierarchical_2019}
% This is another idea that has a correspondence in CNNs
% \cite{zhang_dive_nodate}
% }. In this text, we will refer to neural
% networks that make use of message-passing layers in the style of
% \refeq{gnn-framework} as \ild{Graph Neural Networks}. Although the naming is
% somewhat ambiguous, this is the commonly used keyword.

% introduce neural nets as classifier here

A possible family of models that can be employed for classification are
\textit{neural networks}. We introduce neural networks by example of one of its
simples variants, the \ild{multilayer perceptron}, or \ild{fully-connected
  neural network}
\footnote{A comprehensive introduction can be found in
  \citeauthor{zhang_dive_nodate} \cite{zhang_dive_nodate}
}. We then introduce the notion of convolutions on grid-structured data and
proceed to generalise the intuition to graph-structured data, yielding the class
of Graph Neural Network models. We proceed to describe how GNN models can be
used for node classification.


\paragraph{Neural Networks}

The most basic building block of any neural network is a single
\ild{neuron}, which is given as the composition of a linear transformation with
a nonlinear \ild{activation function} $\sigma$. Formally, for a single input
vector $\vec x_i$, the output of a single neuron is given by $\sigma(\vec x_i^T
\vec w + \vec b_i)$ where $\vec w$ are called the \ild{weights} of the linear
transformation and $\vec b$ is the \ild{bias}. The stacking of multiple neurons,
of which each takes $\vec x_i$ as input, constitutes a \ild{fully-conntected} or
\ild{dense} \ild{layer} in a neural network. If inputs and weights are stacked
in matrices $\mat X$ and $\mat W$, respectively, the output of a single layer is
given by $\sigma(\mat X^T \mat W + \vec b)$ where $\sigma$ is applied
element-wise.
%
A neural network is obtained by stacking multiple layers sequentially such that
the output of one layer is the input of the next layer. The output of an
intermediate layer is often called \ild{latent} or \ild{hidden}
\ild{representation}. Formally, if $\mat H^{(i)}$ is the output of the $i$-th layer,
then the output of the $i+1$-th layer is given as
\begin{align}
\mat H^{(i+1)} = \sigma(\mat (H^{(i)})^T \mat W^{(i+1)} + \vec b^{(i+1)}).
\label{eq:fully-connected}
\end{align}
% TODO sounds too much like what I wrote in the report?
%

\paragraph{Training} The layer weights $\Theta = \{\mat W^{(1)}, ...,
\mat W^{(l)}\}$ are considered to be the model parameters $\theta$ in the sense
of the supervised learning framework described in \refsec{supervised-learning}.
We aim to find a set of parameters such that the empirical risk is minimised.
% TODO empirical risk vs BCE loss function
This objective can be
optimised by \ild{Gradient Descent}: We iteratively adjust the weights $\mat
W^{(i)}$ such that the model prediction is improved with respect to the
empirical risk $\Remp$ \footnote{In the context of neural networks, $\Remp$ is
  also sometimes called the total \ild{loss}. This should not be confused with
  the loss function $\mathcal{L}$.} . More precisely, we consider the partial
derivative of $\Remp$ with respect to each weight matrix, which can be
determined via the chain rule, and iteratively update the weights for a maximum
number of steps or until the risk no longer improves. We can additionally
specify the step size by a given \ild{learning rate} $\eta$. Formally, in each
step, also called \ild{epoch}, we set
\begin{align*}
\mat W^{(i)}' \gets
\mat W^{(i)} - \eta \frac{\partial \mathcal{R}_{\text{emp}}}{\partial \mat W^{(i)}}.
\end{align*}

\paragraph{Hyperparameters} Characteristics such as the number of layers, the
number of neurons in each layer, the choice of loss function and learning rate
determine the \ild{model architecture} and are sometimes called
\ild{hyperparameters}. The proper choice of hyperparameters is essential to the
performance of the model and often determined empirically via search techniques.

\paragraph{Automatic Feature Extraction and Message-Passing}
% TODO override \paragraph to include a . at the end
In practise, selecting suitable features is in itself a substantial
step in the process of finding a well-performing classifier.
%
Particularly in high-dimensional domains, feature selection is not trivial.
Consider the use-case of trying to find a model that, given a grayscale pixel
image of dimensions $(w, h)$, classifies the image based on whether it depicts a
dog or a cat. It is not trivial how to manually extract features (criteria) from
the image that are predictive of the target class. A straightforward approach
would be to consider the brightness value of each input pixel as a feature, i.e.
$\vec x_i \in \mathbb{R}^{w \cdot h}$. However, intuitively, looking at each
pixel in isolation is unlikely to be useful since certainly the target class
depends on the values of pixels \textit{in their context}.
% TODO moreover, extremely expensive if using MLP
Further, note that the hidden representation of some input (e.g. given by
\refeq{fully-connected}) may be considered an alternate feature representation
of this input. In this sense, neural networks can be thought to perform
\ild{automated feature extraction}. 

% TODO also already introduce example of seq2seq transformers here?

\paragraph{Convolutions on Grids}
For sake of intuitive appeal, let us further entertain the example of
classifying pixel graphics
\footnote{
  Analogous formulations can be used for other kinds of structured data,
  including 1-dimensional grids (sequences) such as RNA and DNA sequence alignments
  \cite{flagel_UnreasonableEffectivenessConvolutional_2019}\cite{aoki_ConvolutionalNeuralNetworks_2018}.
}. It is important to note that we have additional
information on the structure of the input,
% (also called \ild{inductive bias}),
% TODO that term is not relevant here? would just sound fancy
namely that its pixel values are arranged in a grid. This means there is a
well-defined notion of context, or \textit{neighbourhood}.
%
Aggregating information across a local neighbourhood may enable
a model to capture not only characteristics of individual pixels but
higher-order patterns. To faithfully extract local patterns, the aggregation
operation should be \ild{local} (the aggregation considers only a part of the
input image) and \ild{translation invariant} (the aggregation should respond
similarly to the same input patch, regardless to where it is positioned in the
entire grid). In the context of neural networks, such an aggregation is called a
\ild{convolution} \cite{zhang_dive_nodate}. In the case of linear aggregation
and grid-structured data, we can express a convolution operation on pixel $\mat
H_{i,j}$ as
\begin{align}
  \mat H_{i,j} = u + \sum_{a= -\Delta}^\Delta \sum_{b = -\Delta}^\Delta \mat V_{a,b} \mat X_{i+a,j+b}
  \label{eq:convolution}
\end{align}
% TODO mention that this requires fewer parameters as fully-connected layer?
% TODO diagram / illustration for grid convolution
where $u$ is the bias, and $\Delta$ is the (window) \ild{size} of the
\ild{convolution kernel} $\mat V$.
%
Note that the kernel can be any
computation adhering to the constraints of locality and translation invariance.
%
\refeq{convolution} describes a
\ild{convolutional layer} and a neural network containing such layers is called
a \ild{convolutional neural network} (CNN).

In image classification, a single input to the neural network is typically an
entire image (a collection of pixels), and the convolution operator is applied
to each pixel. 
%
Successive application of convolutional layers,
potentially with different kernels can be thought to aggregate increasingly
higher-order patterns in the input data. The extracted patterns can be
considered intermediate, higher-order feature representations and the successive
convolution operations extract increasingly higher-order features.
%
Since a kernel computes a value for a given pixel based on the features of
itself and its neighbours, we can interpret the operation of a kernel to perform
\ild{message-passing}: Neighbour nodes construct and transmit messages to the
target node, which are then combined with the target node's features into the
final output.


% TODO note somewhere that 'spectral' GNNs have been around for a while, but
% GNNs have only become quite popular recently with these simplified formulations
\paragraph{Convolutions on Graphs} We extend the notion of a convolution
quite from grid-structured graphs to arbitrary graphs. We will see that
the convolutions considered here are trivially local and translation invariant.
The key differences are that the order and the size of the neighbourhood is no
longer fixed. As such, a potential aggregation must be \ild{node-order
  equivariant}.

Let us consider an attributed graph
$G$ with node set $V$.
% $G=(\mat X, V,E)$ with node set $V$, edge
% set $E$ and $d-dimensional$ attributes $\mat X \in \mathbb{R}^{\norm{V} \times
%   d}$.
% TODO somewhere maybe 'GNNs for Node Classification', make clearer that in NC,
% an input is a single node
Let $\vec h_i$ be the (intermediate) feature representation of vertex
$v_i \in V$. Let $\mathcal{N}(v)$ be some neighbourhood of $v \in V$. Typically,
$\neighb(v)$ is chosen as the 1-hop adjacency in $G$. However, note that
different notions of neighbourhood can also be applied, so the input graph must
not necessarily correspond directly to the computation graph that defines how
messages are passed \footnote{ For instance, \textsc{GraphSAGE}
  \cite{hamilton_InductiveRepresentationLearning_2018} samples a fixed number of
  adjacent nodes. }. We can describe a simple convolution operation on
attributed graphs as follows: the new latent representation $\vec h_i'$ of $v_i$
is based on messages received by its neighbours. Each neighbour encodes its
features by means of a function \textsc{Msg}. These messages are
aggregated using a permutation-invariant aggregation function \textsc{Agg}.
% TODO invariant or equivariant?
Finally, the neighbours' input and the node's own features $\vec h_i$ are
coalesced via an update function \textsc{Update} into the new latent
representation $\vec h_i'$. To summarise:
\begin{align}
  & \vec h_i' \gets \textsc{Update}(\vec{msg}_{ii}, \textsc{Agg}(\{\textsc{Msg}(\vec h_j, \vec h_i) ~|~ j \in \mathcal{N}_i\}))
    \label{eq:gnn-framework}
\end{align}
Concrete choices of \textsc{Msg}, \textsc{Agg} and \textsc{Update} give
implementations of \ild{graph convolution layers}. A neural network containg
such layers is commonly called a \ild{graph neural network} (GNN)
or \ild{graph convolutional network} (GCN).

In simple GNN architectures, \textsc{Msg} is a linear feature extraction and depends
only on the sending node, i.e. $\textsc{Msg}(\vec h_j, \vec h_i) = 
\mat W \vec h_j =: \msg_j$ for some learnable weight matrix $\mat W$. Note that
like the kernel parameters $\mat V$ in \refeq{convolution}, these weights are
shared among message-passing steps.
% TODO should this be a matrix or a vector? pretty sure matrix but doesnt hut to check

\textsc{Update} is the application of a non-linear activation function and
\textsc{Agg} is given by
\begin{align*}
  & \textsc{Agg}(...) = \bigoplus_{j \in \mathcal{N}_i} \alpha_{ij} \vec{msg}_j,
\end{align*} where $\alpha_{ij}$ is a coefficient determing the importance of
$\vec{msg}_{j}$.


The framework of \refeq{gnn-framework} produces a hidden feature
representation for each node. This is the basis for solving node-level tasks
such as the classification of clustering of individual nodes. Another common use
case is to make a prediction for the entire input graph, the most prominent
example in context of life sciences being molecule graphs. In this case, we want
to aggregate all node features to produce a single value describing the input
graph. Such an aggregation is called a \ild{pooling} layer. This can be done
either by application of a simple permuation-invariant
% TODO check lingo here
aggregation function such as minimum, maximum, etc. or by iteratively coarsening
the graph together with its node-level feature representations \cite{ying_hierarchical_2019}.

% TODO also mention edge-level tasks and computation over edges,
% see https://distill.pub/2021/understanding-gnns/#modern-gnns > modern GNNs >
% thoughts for some references

\paragraph{Simple GNN} The \ild{GCN layer} \footnote{The naming here is unfortunately ambiguous.} as
proposed by \citeauthor{kipf_semi-supervised_2017}
\cite{kipf_semi-supervised_2017} defines $\alpha_{ij}$ as a constant depending
on the degrees of $v_i$ and $v_j$, namely $\alpha_{ij} := \nicefrac{1}{\sqrt{d_i
    d_j}}$.
% TODO elaborate?

\paragraph{GNNs with Attention} The \ild{Graph Attentional Layer} (GAT)
\cite{velickovic_graph_2018} allows the importance score to be learnable, i.e.
adjusted via backpropagation and gradient descent during network training. An
attention mechanism $A$ determines the importance $e_{ji}$ of $\vec{msg}_{ji}$.
If the neighbourhood $\mathcal{N}_i$ is defined as the 1-hop-neighbourhood in
the input graph, this can also be interpreted as the importance of edge $(v_i,
v_j)$. In its prototypical formulation, $A$ is a single-layer fully-connected
neural network with learnable weights $\vec a$ and activation function
$\sigma_{\text{att}}$, as described in \refeq{fully-connected}. $A$ receives
both the feature representations of $v_i$ and $v_j$ as input, combined by
concatenation:
\begin{align}
  e_{ij} := A(\msg_i, \msg_j) = \sigma_{\text{att}}(\vec a^T( \msg_i \concat \msg_j))
\end{align}
Importance scores $e_{ij}$ are then normalised via the \ild{softmax} function:
\begin{align}
  \alpha_{ij} := \softmax_{j \in \mathcal{N}_i}(e_{ij}) := \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}.
  \label{eq:softmax}
\end{align}
% TODO: mention multi-head attention?
Attention is a technique that has previously been used successfully in other
neural network architectures. In particular, Graph Attention Networks are
analogous to the Transformer architecture \cite{vaswani_AttentionAllYou_2017}
that has achieved great popularity particularly in areas of Natural Language
Processing such as machine translation. For translation from a source language
to a target language, text input is typically given as two sequences of tokens.
The key idea behind the Transformer architecture in NLP is to apply an attention
mechanism to assess the importance of other words in the source or target
sequence with respect to the current word.
% TODO diagram like in https://docs.dgl.ai/tutorials/models/4_old_wines/7_transformer.html
%   to illustrate message-passing of transfomer?
Note that while the Transformer architecture relies heavily on the attention
mechanism, it also directly implements the idea of message-passing based on
known relationships between atomic inputs.

% TODO inherent advantages of GNNS:
% - work with graph structure -- two notions:
%   - explicit relationships between data points
%   - 'data points' themselves are graph-structured, can capture this and is
%   naturally invariant of order, rotation, etc, not like winter2018
%   - (graph-level tasks): independent of size of input graph (no fixed-length encoding)
%      - naturally does not consider rotations etc.


% TODO GNNs relatively low complexity, seem to not need super-deep convolutions like w/images


% TODO dont go into polynomial filters for graphs here but mention them in
% related work, the distill article gives a nice journey there that we can
% transcribe: https://staging.distill.pub/2021/understanding-gnns/

\begin{figure}[h]
  \centering
  (diag-surfaces) on paper
  % TODO diagram:
  % -- features in isolation
  % — convolution on grid (like in dive 6.2.1)
  % — 'convolution on sequence' (like in Transformer)
  % can we find biological sequence here?
  % — convolution on graph
  % — neighbourhood on some other shape (maybe 3D shape)
  % see (diag-surfaces) on paper
  \caption{Including known structure into the ML model as predictive bias.}
  \label{fig:diag-surfaces}
\end{figure}




% TODO are we mentioning this?
% successive layers of different convolution kernels detect features of
% increasingly higher order (edges, simple features, complex features)
% approach relies on a notion of neighbourhood. a pattern is specific
% configuration *in a specific neighbourhood*

% having this geometry is an additional information/bias we give to the network/model

% can consider more complex geometries/surfaces → gauge-equivariant models
% allow to properly rotations etc (``respect underlying symmetries''')

% the task is usually
% defined on the entire geometric object (image)
% — but possibly also 'node'-scale, like saliency maps?
% starting point: https://medium.com/stellargraph/https-medium-com-stellargraph-saliency-maps-for-graph-machine-learning-5cca536974da

% in case of graphs, this geometry, notion of neighbourhood, is given by graph
% adjacency.
% - images can be considered grid graphs
% - task can be defined on whole graph, nodes or even edges or intermediate scales.
% - node-level, edge-level, graph-level; or on intermediate scale, finding
% communities, subgraphs, motifs, ... 

\paragraph{Neural Networks as Classifiers} Intermediate layers of neural
networks can be interpreted to compute useful feature representations. One way
to obtain a classification prediction from a neural network is to use these
intermediate representations as an input to another off-the-shelf classifier
(such as Support Vector Machines, for instance
\cite{liu_CombiningConvolutionalNeural_2018}). Another, more common approach,
however, is to design the network such that the last layer contains one ouput
neuron for each target class. Given some input $\vec x$, the output value of the
$i$-th neuron expresses the estimated probability that $\vec x$ is of class
$c_i$. To produce values in $[0,1]$, the results of the final layer are
normalised, e.g. with the \textit{softmax} function (see \refeq{softmax}).

In order to train the neural network for classification, we need a loss function
$\mathcal{L}$ to assess the quality of the prediction. We introduce the
\ild{Binary Cross Entropy} loss from the viewpoint of maximum likelihood
estimation (based on \cite{zhang_dive_nodate}) but note that it can also be
derived via the notion of cross-entropy.
%
Generally speaking, we seek to estimate parameters $\Theta$ of an assumed
probability distribution $P$ given some observed data $X$. The method of
\ild{maximum-likelihood estimation} postulates that we should pick the
parameters such that the observed data is most likely (i.e. occurs with highest
probability) under $P$. The likelihood \wrt parameters $\Theta$ is measured by
a \ild{likelihood function} $L(\Theta)$.
Formally, we aim for
% TODO use underset (change \argmax command)
\begin{align}
  \hat \Theta = \argmax_\Theta P(X~|~\Theta) = \argmax_\Theta L(\Theta)
\end{align}
Because of connections to entropy and computational convenience, we instead
consider the negative logarithm:
\begin{align}
  \hat \Theta = \argmin_\Theta -\log L(\Theta)
\end{align}
Looking at the negative log-likelihood in more detail directly yields the
\ild{Binary Cross-Entropy loss}.
For notational convenience, let $\mathcal{Y} =
\{0,1\}$. Let $\pi_i = P_\Theta(y_i=1~|~\vec x_i)$ be the estimated probability
that $\vec x_i$ belongs to class $y_i$. Since we are dealing with binary
classification, we have $P_\Theta(y_i=0~|~\vec x_i) = 1 - P_\Theta(y_i=1~|~\vec
x_i)$. Under the assumption that input samples are independent and identically
distributed, we have
% TODO check where else we can use align* instead of align
\begin{align*}
  -\log L(\Theta)  &= -\log \prod_{i=1}^n (\pi_i)^{y_i} \cdot (1-\pi_i)^{1-y_i} \\
  &= - \sum_{i=1}^n y_i \log(\pi_i) + (1-y_i) \log(1-\pi_i) \\
  &= \sum_{(\vec x,y) \in \mathcal{T}} \text{CE}(f(\vec x), y)
    \label{eq:bce-loss-basic}
\end{align*}
where $\text{CE}$ is the cross entropy loss and $\mathcal{T}$ is the
set of training data. In summary, minimising the cross entropy loss is
equivalent to maximising the likelihood.
% TODO other theta than model parameters -- confusing


\section{Support Vector Machines}
% TODO define what a SVM is 

% things that should be clear:
% - basic intuition
% - cost parameter C
% - kernels
% - 'gamma' and class weights?

Support Vector Machines (SVMs) are a family of supervised machine learning
models typically used for binary classification. In this section, we present the
basic derivation of SVMs with the motivation of providing intuition behind the
\textit{cost} hyperparameter and choice of kernel functions and their parameters. We
refer to
\citeauthor{tibshirani_ElementsStatisticalLearning_2017}
\cite{tibshirani_ElementsStatisticalLearning_2017} for a more rigorous
and thorough treatment.

Support Vector Machines are linear classifiers: 
The basic idea is to find a hyperplane in the (possibly transformed) feature
space $\mathcal{X}$ that best separates the training data \wrt its
ground-truth class assignments.

% TODO mention what this derivation is based on

\paragraph{Preliminaries} For sake of notational convenience, let $\mathcal{Y} =
\{-1, 1\}$. A \ild{hyperplane} is an affine set of points $L := \{\vec x~|~ \vec
x^T \vec \beta + \beta_0 = 0\}$. The signed distance from a point $\vec x$ to $L$ is
% TODO make it clearer that beta is a vector?
given by $d(x, L) := \nicefrac{1}{\norm\beta} (\vec x^T \vec \beta + \beta_0)$. Based
on $L$, we can define a linear classifier
\begin{align}
  h_L(\vec x) = \sign(\vec x^T \vec \beta + \beta_0).
  \label{eq:linear-classifier}
\end{align}

$\mat X$ is \ild{linearly separable} if there exists a hyperplane
$L$ such that $h_L(\vec x_i) = y_i$ for all $\vec x_i \in \mat X$. $L$ is then
called a \ild{separating} hyperplane, or \ild{decision boundary}.

\paragraph{Linearly separable case} For now, assume that $\mat X$ is linearly
separable. We wish to find a suitable separating hyperplane. One possible
approach would be to minimise the distance of misclassified points to the
hyperplane. Doing so by gradient descent yields the \ild{perceptron training
  algorithm}. However, we are not guaranteed a unique solution. Further, if
$\mat X$ is not linearly separable, the algorithm will not converge at all.
Another possible approach is to search for a hyperplane that maximises the
\ild{margin} $\gamma(L)$ \wrt $\mat X$, i.e. the distance from the hyperplane to the
closest point:
\begin{align}
  \gamma(L) := \min_{x \in \mat X} \nicefrac{1}{\norm\beta} \abs{ \vec x^T \vec \beta + \beta_0 }.
\end{align}
The maximum-margin separating hyperplane is unique. Further, it seems reasonable
to assume that a maximum-margin separating hyperplane will do well in
generalising to unseen points. We aim to find a hyperplane $L(\vec \beta,
\beta_0)$ that achieves:
% TODO elaborate the constraint
\begin{maxi}{\vec \beta, \beta_0}{\gamma(\vec \beta, \beta_0)} {\label{eq:svm-1}}{}
  \addConstraint{y_i (\vec x_i^T \beta + \beta_0) \geq 0; i = 1, \ldots, n}
\end{maxi}

Since $L$ and $\gamma$ are scale invariant, i.e. $\gamma(\vec \beta, \beta_0) =
\gamma(\lambda \vec \beta, \lambda \beta_0)$ for any $\lambda \not= 0$, we can
assume $\gamma(\vec \beta, \beta_0) = \nicefrac{1}{\norm \beta}$, i.e. 
$\min_{x \in X} \abs{\vec x^T \vec \beta + \beta_0} = 1$.
\refeq{svm-1} is then equivalent to

\begin{maxi}{\vec \beta, \beta_0}{\nicefrac{1}{\norm{\vec \beta}}}
  {\label{eq:svm-2}}{}
  \addConstraint{y_i (\vec x_i^T \beta + \beta_0) \geq 0; i = 1, \ldots, n}
  \addConstraint{\min_{x \in X} \abs{\vec x^T \vec \beta + \beta_0} = 1}
\end{maxi}
and can simplified further to
\begin{mini}{\vec \beta, \beta_0}{\norm \beta} {\label{eq:svm-3}}{}
  \addConstraint{y_i (\vec x_i^T \beta + \beta_0) \geq 1; i = 1, \ldots, n}
\end{mini} 


\paragraph{General case} Let us now consider the case that $\mat X$ is not
linearly separable. In this case, there is no solution to the optimisation
problems given above. However, we can relax the constraints so that some
data points are allowed to lie inside the margin. We achieve this by introducing
\ild{slack variables} $(\xi_1, ..., \xi_n)$ to the optimisation constraints
that express the allowed violation:

\begin{mini}{\vec \beta, \beta_0}{\norm \beta} {\label{eq:svm-slack}}{}
  \addConstraint{y_i(\vec x_i^T \vec \beta + \beta_0)}{\geq 1 - \xi_i}
  \addConstraint{\xi_i > 0}
  \addConstraint{\sum_{i=1}^{n} \xi_i \leq K}
\end{mini} 
where $K$ is some constant.  

This optimisation is effectively solved by first transforming it into an
equivalent problem known as its $\ild{Langrangian Dual}$. To this end, it is
convenient to express \refeq{svm-slack} as

\begin{mini}{\vec \beta, \beta_0}{\nicefrac{1}{2} \norm{\beta}^2 + C \sum_{i=1}^n \xi_i} {\label{sq:svm-slack-2}}{}
  \addConstraint{y_i(\vec x_i^T \vec \beta + \beta_0)}{\geq 1 - \xi_i}
  \addConstraint{\xi_i > 0}
\end{mini} 

The \ild{cost} hyperparmeter $C$ can be interpreted as a tradeoff coefficient
between the cost of margin violations and simplicity of the decision boundary.
For large $C$, margin violations will be punished more strictly. For small $C$,
violations may be allowed to achieve a hyperplane with lower norm, i.e. smoother
decision boundary.


\paragraph{The Kernel Trick} For most non-trivial classification problems, the
classification function $f$ we seek to approximate is not linear. A method to
move beyond linearity but nevertheless use linear classifiers is to
transform the input features $\mat X$ into a higher-dimensional space
$\mathcal{X}'$ via some transformation $\Phi$. Depending on the choice of
$\Phi$, $\mathcal{X'}$ may be of very high or even infinite dimensionality.
Thus, computing $\Phi$ explicitly is sometimes not an option. Luckily, we will
see that in case of Support Vector Machines, all we need is an inner product
$\inner{\cdot}{\cdot}$ in $\mathcal{X}'$. In the following, instead of some
feature vector $\vec x \in \mathcal{X}$, we consider a transformed feature
vector $\Phi(\vec x) \in \mathcal{X} '$.
%
Let's inspect the most central equations for computing SVMs. First, consider the
Lagrangian Dual of \refeq{svm-slack-2} given by
\begin{align*}
  L_D = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j
  \inner{\Phi(\vec x_i)}{\Phi(\vec x_j)} \\
  \text{where~~~} & \alpha_i = C - \mu_i \\
  & \forall i:~~ \alpha_i, \mu_i \geq 0 \\
\end{align*}
% TODO formatting
% TODO what is y_i here?
% TODO omitting KKT conditions, is this a problem?
Further, the decision function $h_L$ (see \refeq{linear-classifier}) can be
rewritten as
\begin{align*}
  h_L(\Phi(\vec x)) & = \Phi(\vec x)^T \vec \beta + \beta_0 \\
              & = \Phi(\vec x)^T \left[ \sum_{i=1}^n \alpha_i y_i \Phi(\vec x_i) \right] + \beta_0 \\
  &  = \sum_{i=1}^n \alpha_i y_i \inner{\Phi(\vec x)}{\Phi(\vec x_i)} ~~ + \beta_0
\end{align*}
% TODO include sign here or not use sign in definition

The key point here to note is that $\Phi$ appears only in the context of inner
products. This means we can avoid even specifying $\Phi$ explicitly and instead
use a \ild{kernel function} $K$ that expresses the inner products in the
transformed feature space:
\begin{align*}
  K(\vec x, \vec x') = \inner{\Phi(\vec x)}{\Phi(\vec x')}.
\end{align*}
$K$ is a valid kernel function if its Gram matrix is positive semidefinite.

One popular choice is the \ild{radial basis function} (RBF) kernel\footnote{This
  is in fact an example where the transformed feature space is of infinite
  dimension: It can be seen via Taylor expansion that \Krbf is based on an
  infinite sum of polynomial kernels.
  % TODO to include this, need to cite
  % http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf
  % or similar and define polynomial kernels (but as <x, x'>^n would suffice?)
}, also called \ild{Gaussian} kernel. Recall that a kernel function can be
interpreted as a measure of similarity between its two arguments $\vec x$ and
$\vec x'$. The RBF kernel implements this notion as a decaying function of their
distance. If $\vec x$ and $\vec x'$ are similar, their distance will be small and $-
\gamma \norm{\vec x - \vec x'}^2$ will be relatively large. The decaying
characteristic is implemented by the application of the exponential function.
Thus, we can define the RBF kernel as
\begin{align}
  K_{\text{RBF}} (\vec x, \vec x') = \exp( -\gamma \norm{\vec x - \vec x'}^2 ).
  \label{eq:rbf-kernel}
\end{align}
Since the distance is symmetric, $\Krbf$ can be interpreted as a bell-shaped
function.
% The effect of using $\Krbf$ can be interpreted visually in low
% dimensions as adding 
The hyperparameter $\gamma$ controls the width of the bell shape, with
larger values producing a more narrow shape. 
% TODO mention some other kernels
% TODO diagram like here, with 3 points
% https://medium.com/analytics-vidhya/radial-basis-functions-rbf-kernels-rbf-networks-explained-simply-35b246c4b76c

% TODO explain how we can get probabilities as output



\section{Evaluation of classifiers}
% TODO move this to methods (pretty sure we should)

SVMs as well as GNNs are trained to optimise some specific function \wrt the
training data. However, when comparing and selecting models, we need an unbiased
performance measure that is based only on data and ground-truth labels.

The classifiers considered here will output a probability (or \ild{confidence
  score}) that a given data point will belong to the positive class.
% TODO elaborate how SVM can output probas
To obtain a concrete, binary classification, we have to draw a
\ild{decision threshold} $\tau$. If the predicted confidence score of a
given data point is greater than $\tau$, it will be assigned the positive class,
else the negative class.
%
In \reftab{conf-names} we introduce some basic terminology for subsets of
training data based on their true and predicted class.
\begin{table}[h]
  \centering
  \begin{tabular}[h]{l | l l}
    & Predicted Positive (\PP) & Predicted Negative (\PN) \\
    \hline
    Actually Positive (\P) & \ild{True Positives} (\TP) & \ild{False Negatives} (\FN) \\
    Actually Negative (\N) & \ild{False Positives} (\FP) & \ild{True Negatives} (\TN)
  \end{tabular}
  \caption{Given a concrete, binary classification, the following terms describe
  (sizes of) subsets of the predicted data, depending on its ground-truth and predicted class.}
  \label{tab:conf-names}
\end{table}
% TODO formatting (space after command)

Based on the terms in \reftab{conf-names}, we can define the following measures:
% TODO reference for this
\begin{itemize}
\item \ild{Accuracy}, given by the ratio of
  correctly classified examples: $\nicefrac{\TP + \TN}{\P+\N}$. Note that for
  class-imbalanced data, this is not a sufficient measure, because
  misclassifications of the minority class will be underrepresented in the
  overall score.
\item  \ild{True Positive Rate} (\TPR), or \ild{Recall}, given by $\nicefrac{\TP}{\P}$ is
  the probability that a positive example will be predicted as such by the classifier.
\item \ild{False Positive Rate} (\FPR), given by $\nicefrac{\FP}{\N}$ is the
  probability that a negative example will be predicted falsely as positive.
  \item \ild{Precision}, given by $\nicefrac{\TP}{\PP} = 1 - \FPR$
\end{itemize}

A perfect classifier would yield a high True Positive Rate and a low False
Positive Rate.
%
Note that it is possible to trade-off \FPR and \TPR
by varying the decision threshold. If the threshold is very high, only examples
for which the classifier has high confidence will be actually assigned positive
class. This means that the \FPR will be low. However, then not all true
positives may be picked up as such by the classifier, resulting in a low \TPR.
Lowering the decision threshold will increase \TPR, but also potentially result
in additionally picking up false positives, increasing the \FPR.

The choice of the proper decision threshold,
% i.e. the tradeoff between Recall
% and Precision
depends on the use-case since different importance may be assigned
to either Precision or Recall.
%
The original use-case of \nielsen was to use a classifier trained for predicting
node duplication to provide a low number of high-confidence examples as
suggestions to the user. When considering only the few examples with the highest
score, Precision is more important than Recall.

To assess the trade-off between \FPR and \TPR \wrt possible choices of
classification threshold $\tau$, we can plot \FPR and \TPR as a function of
$\tau$, yielding the \ild{Receiver Operating Characteristic} (ROC curve). The
curve of random classifier that flips an even coin for each given example would
be close to the diagonal. 
%
Generally, a classifier could be considered better if its ROC curve leans
towards the upper left, i.e. the area under the curve is greater.

As a heuristic for choosing $\tau$, we can look for the threshold with the greatest
distance between \TPR and \FPR at $\tau$, i.e. $\tau_{\text{opt}} := \argmax_{\tau \in
  (0,1)} \TPR(\tau) - \FPR(\tau)$.

As an indicator for overall quality, we can compute the \ild{Area Under Curve}
(AUC) score given as
\begin{align*}
  \text{AUC}(\tau) = \nicefrac{1}{2} (\TPR(\tau) - \FPR(\tau) + 1)
\end{align*}
and define the optimal overall AUC score as $\text{AUC} := \text{AUC}(\tau_{\text{opt}})$.
However, we acknowledge that reducing the performance of a classifier to a
single number will hardly ever capture all characteristics and evaluation still
depends heavily on the use-case.



% ====================================
\chapter{Related Work}
\label{sec:related-work}

% TODO chapter intro

We directly extend the work of \citeauthor{nielsen_MachineLearningSupport_2019}
\cite{nielsen_MachineLearningSupport_2019}. In this work, the authors aim to
train a classifier to predict node duplication in a ``human-like'' manner. They
focus on features based on the graph structure. For the classification model,
Support Vector Machines are chosen and the authors tune the model to find
optimal choices of kernel and hyperparameters. The primary use-case considered
in their work is to provide a small, human-digestible number of predictions as
suggestions to the curator. Likewise, a similar approach is used to suggest
de-duplication operations. The SVM model is trained on a sequence of curation
snapshots provided by an expert working on \textit{AlzPathway}, a disease map
describing processes related to Alzheimer's Disease (see \refsec{datasets}).
Model hyperparameters were tuned and selected based on evaluation on
\textit{PDMap}, a diagram on Parkinson's Disease. The final identified model was
further validated on \textit{ReconMap}, a large diagram describing the human
metabolism and a number of smaller pathway graphs obtained from Reactome
\cite{joshi-tope_ReactomeKnowledgebaseBiological_2005}. Finally, a user study
among domain experts was conducted on smaller networks, comparing the
performance of the machine learning model
% TODO captialisation?
with the decisions of experts. We reproduce some of their main results in
\refsec{experiments-results} and discuss them further here. In general, the
identified model was able to make predictions well above random. However, the
user study showed considerable variations even in expert decisions.



\section{Drawing Biological Networks}

There are numerous general-purpose methods for laying out graphs. Examples are
force-directed approaches \cite{kobourov_ForceDirectedDrawingAlgorithms_2013},
stress-based approaches \cite{gansner_GraphDrawingStress_2005}, hierarchical or
layered approaches \cite{healy_HierarchicalDrawingAlgorithms_2013} or orthogonal
or grid-based drawings \cite{duncan_PlanarOrthogonalPolyline_2013}, to name just
a few. Drawing biological networks, however, is often associated with
domain-specific additional requirements. Some of these challenges are outlined
in \refsec{draw-biol-netw}. To this end, there has been work done to develop
methods specific methods for biological networks.

Early work focussed mainly on the drawing of process diagrams that contain up to
several dozens of nodes, e.g. describing only a single biological pathway.
\cite{becker_GraphLayoutAlgorithm_2001}
\cite{schreiber_ComparisonMetabolicPathways_2003}.
% Widely used online databases such as \toolname{KEGG} and \toolname{Reactome}
% provide (semi-)automatic layouts of pathway diagrams.

However, the networks we consider in this work are of the scale of several
hundreds or even thousands of elements. This poses unique challenges such as
respecting a given hierarchy or annotated categories (subsystems), representing semantic
motifs (e.g. cyclic patterns) or keeping a uniform distribution of visual
elements across the drawing.

Recent work \cite{siebenhaller_HumanlikeLayoutAlgorithms_2020,
  kieffer_HOLAHumanlikeOrthogonal_2016} has clarified the domain
specific-requirements of drawing biological process diagrams and suggested
specialised drawing algorithms for medium-scale diagrams.

There are two recent notable approaches which we highlight in more detail.
With \toolname{Metabopolis}, \citeauthor{wu_MetabopolisScalableNetwork_2019}
provide an approach based on the inherent hierarchies provided either by
explicit subsystem annotations or related connected components
\cite{wu_MetabopolisScalableNetwork_2019}.
% TODO this is super vague
%
Shortly thereafter, \citeauthor{wu_MultilevelAreaBalancing_2020} work towards
alleviating the uneven space space utilisation of \toolname{Metabopolis},
focussing on obtaining a layout that has balanced space utilisation at multiple
levels of detail \cite{wu_MultilevelAreaBalancing_2020}. Further, they
explicitly consider node duplication as a tool to improve the layout.
% TODO fuck I CBA to read these entire fucking papes, ill never be done....



\section{Node Duplication}
% TODO this section


% TODO wu2020 (even cites nielsen) -- only preprint


\paragraph{Related Notions} Several related terms appear in the literature that
are relevant for the problem at hand. A \ild{currency metabolite}
\cite{huss_CurrencyCommodityMetabolites_2007}
% TODO also external/internal mtbs cf Schuster
is a metabolite that plays only a secondary
role in most of the reactions it is involved in. This role may be to merely
supply energy (i.e. act as ``currency'' in the metabolism) or act as some other
form of catalyst. Commonly, currency metabolites appear in abundance in an
organism, and often it is assumed that two reactions involving the same currency
metabolite are not in fact linked stoichiometrically, i.e. linking these
reactions via a common node would imply false connectivity.
% Prominent examples
% are molecules such as ATP or H2O.
% NOTE commenting this out since read in places that ATP can actually be a key connector
Currency metabolites are commonly duplicated.
% TODO  also consider direks_dynamic?

In the context of decomposing a metabolic into subsystems,
\citeauthor{schuster_exploring_2002} classify a given metabolite as
\ild{external} if it can be considered in to be present in such abundance that
its production or consumption has no meaningful effect on the surrounding
concentration
\cite{schuster_exploring_2002}.
Note that this means that the connectivity is not meaningful
(\ild{false}) in a stoichiometric sense. 

The notion of true connectivity is also reflected in the concept of \ild{key
  connectors} \cite{kim_IdentificationCriticalConnectors_2019}: because pathways
in a metabolic network seldomly work in isolation, there necessarily will be
connections between different pathways. Determining whether a bridging node
between two pathways is indeed a key connector or merely describes false
connectivity is not trivial and related to the problem at hand.

\paragraph{General methods} While node duplication (also referred to as
\ild{Vertex Splitting}) has been treated from a theoretical perspective
\cite{liebers_PlanarizingGraphsSurvey_2001,abu-khzam_ClusterEditingVertex_2018},
to the best of our knowledge there are relatively few concrete, general-purpose
algorithms that employ automatic node duplication.

Most notably, \citeauthor{eades_VertexSplittingTensionfree_1996} introduce a
% TODO do not list all names in \citeauthor
general graph drawing algorithm that applies node duplication to simplify the
graph structure in order to find a better layout
\cite{eades_VertexSplittingTensionfree_1996}. The method is an extension of the
force-directed \textsc{Kamada-Kawai} algorithm
\cite{kamada_AlgorithmDrawingGeneral_1989}. Given a vertex $v$ and an incident
edge, they define a \ild{tension} vector whose direction is the direction of the
edge in the current drawing and whose magnitude is based on the difference
between the actual edge length in the current drawing and some target distance.
Any possible binary duplication defines a \ild{split line} that partitions
$\mathcal{N}(v)$ into two disjoint subsets $\neighb(v)^+$ and $\neighb(v)^-$.
The \ild{tension of a split line} is the sum of tensions of either
$\neighb(v)^+$ or $\neighb(v)^-$ (they are in fact equal if the drawing is
in equilibrium). The \ild{tension of a vertex} then is the maximum tension of all
possible split lines. In each iteration of the \textsc{Kamada-Kawai} algorithm,
additionally, a vertex with tension greater than a given threshold is split
into two duplicates according to its maximum tension split line. The choice of
the threshold is left to the user.
% TODO this is a bit elaborate.

Node Duplication has also been applied in the area of Electronic Circuit Design,
for example to avoid edge crossings \cite{li_EliminateWireCrossings_2008} or
paths exceeding a maximum length
\cite{paik_VertexSplittingDags_1998, mayer_GeneticAlgorithmsVertex_1993}.

\citeauthor{henr_ImprovingReadabilityClustered_2008}
 consider how to represent
duplicates in a social network visualisation in which communities are
represented by adjacency heatmaps
\cite{henr_ImprovingReadabilityClustered_2008}.


\paragraph{In biological networks} We review related work which explicitly
considered node duplication in biological networks. The following references
mainly consider the drawing of metabolic networks. However, the referenced
methods exclusively rely on structural criteria based on the graph structure.

% % \paragraph{Metabolic Models}
% % TODO introduce somewhere what that is?
% % TODO some introduction text
% Because different instances of the same metabolite can participate in many
% different reactions, node duplication can be essential for working with
% metabolic models.
% % not sure about this
% % While visualisation is one possible motivation, node duplication
% % also has to be considered for other tasks such as flux balance analysis
% % % TODO really?
% or integration of secondary \textit{-omics} data \cite{manipur_clustering_2020}.

% Node duplication in layout tasks potentially is a different problem from the
% general preprocessing since finding a good layout may come with particular
% constraints (such as considering edge crossings, stress etc.). However, the
% methods mentioned below do not take layout information into account when
% deciding node duplication and we thus make no explicit distinction here.
%

% already saying this in Background
% Any node $v$ with degree greater than two introduces a connectivity between any
% two subsets of $\neighb(v)$. It is not trivial to distuingish whether a
% connectivity is \ild{false}, i.e. only exists because participants in two reactions
% happen to be mapped to the same concrete node
% % TODO terminology, here we are arbitrarily shifting from maths to disease map lingo
% or \ild{true} in the sense that the connectivity represents actually meaningful
% biological information.


A common intuition is that a node shall be duplicated if its
neighbourhood is highly heterogenous \wrt some similarity measure. In other words, a
node should be split if it is involved in many different, unrelated processes
as then it is assumed that in reality there are independent instances of that
species, involved in the different processes independently.
% TODO what we are saying here is good, but sentence sounds pretty awkward

One possible approach is to consider a graph-based centrality measure. If the
target node has a very high centrality score, we conclude that 
the node must then necessarily be involved in different, unrelated processes, i.e.
its neighbourhood is heterogeneous. For a concrete choice of centrality measure,
early methods simply considered the node degree
\cite{ma_ReconstructionMetabolicNetworks_2003,schuster_exploring_2002}.
Further work uses the Eigenvector centrality \cite{manipur_clustering_2020}.

Other approaches make the notion of neighbourhood heterogeneity more explicit by
characterising communities in the given network. A node then has heterogeneous
neighbourhood, if it is incident to many different communities. Communities can
be determined solely on the graph structure, for instance as induced by
modularity maximisation \cite{newman_modularity_2006}.
\citeauthor{huss_CurrencyCommodityMetabolites_2007} decide node duplication
based on the contribution to overall modularity if the target node is removed
\cite{huss_CurrencyCommodityMetabolites_2007}.
\citeauthor{guimera_FunctionalCartographyComplex_2005} classify nodes as
different kinds of hub- or connector nodes based on their intra- and
inter-community degrees, where communities are determined via modularity maximisaiton
\cite{guimera_FunctionalCartographyComplex_2005}. Communities can also be
characterised by domain-specific biological knowledge, i.e. annotations that
describe the cellular compartment \cite{manipur_clustering_2020} or the pathway 
\cite{rohrschneider_NovelGridBasedVisualization_2010}
\cite{joshi-tope_ReactomeKnowledgebaseBiological_2005} for a given node.

Recently, in the context of visualising large-scale diagrams such as the disease
maps considered herein, \citeauthor{wu_MultilevelAreaBalancing_2020} propose the
following scheme for deciding whether to duplicate a node. Nodes are categorised
as \ild{important} or \ild{unimportant} based on a whether their degree exceeds
a certain degree threshold. Alternatively, the categorisation may be given by an expert.
% 
Important vertices are duplicated such that they appear at most once in each
cluster. Unimportant vertices are duplicated such that each duplicate has degree
of exactly one.
%
% additionally: 1. steiner tree vis connecting duplicates

Apart from the work of \citeauthor{nielsen_MachineLearningSupport_2019}, we are
not aware of any other previous work that investigates using a machine learning
classifier for node duplication.


\paragraph{Tools and Formats}
There are several tools that provide functionality to easily introduce
duplicates once the decision has been made. \toolname{Arcadia}
\cite{villeger_ArcadiaVisualizationTool_2010} allows to split a node such that
each copy has exactly unit degree. \toolname{Omix}
\cite{droste_OmixVisualizationTool_2013} makes it easier to introduce duplicates
with certain connectivity patterns by providing a \textit{motif stamp} tool.
% TODO description is lacking
%


\section{Disease Maps}
% TODO update nameref s.t. name is put in quotation marks
To date, disease maps have been created for a number of diseases, including
Alzheimer's Disease (\alzpathway \cite{ogishima_AlzPathwayUpdatedMap_2016}),
Parkinson's Disease (\pdmap \cite{fujita_IntegratingPathwaysParkinson_2014}),
and recently \textsc{COVID-19}
\cite{ostaszewski_COVID19DiseaseMap_2020}.

Beyond serving as a platform for
integrating existing knowledge, computational methods have been applied to,
e.g., identify molecules and relations essential for the pathogenesis of Alzheimer's
Disease \cite{mizuno_NetworkAnalysisComprehensive_2016}.

The \textit{Atlas of Cancer Signalling Network} (ACSN)
\cite{kuperstein_AtlasCancerSignalling_2015} is a collection of diagrams
describing signalling processes related to cancer. Recent work
\cite{sompairac_metabolic_2019} has linked ACSN and \textit{ReconMap}, making
the connections between common species in both networks explicit. This is of
particular interest since ACSN focusses on signalling processes, while
\textit{ReconMap} describes metabolic processes.
Additionally, the information from \textit{ReconMap} is used to augment the ACSN diagram.


% TODO mention minerva?

% TODO applications
% cf Obsidian 'Alzheimers Disease Maps' Applications



Several specialized tools exist for the curation and exploration of large
biological process diagrams, including \toolname{CellDesigner}
\cite{funahashi_CellDesignerVersatileModeling_2008}, \toolname{Minerva}
\cite{gawron_MINERVAPlatformVisualization_2016}, \toolname{NaviCell}
\cite{kuperstein_NaviCellWebbasedEnvironment_2013} and \toolname{Cytoscape}
\cite{shannon_cytoscape_2003} and \toolname{VANTED}
\cite{rohn_VANTEDV2Framework_2012}.
%
One of the most common formats used for describing disease maps is an extension to SBML
Level 2 given by \toolname{CellDesigner}. Further, SBML Level 3 now allows to attach
layout information. Another prominent format is SBGN-ML.

Detailed information on the disease maps considered in this work can be found in
\refsec{datasets}.

% TODO that one pape inferring ontology from disease map



% -------------------------------------
\section{Graph Neural Networks in the Life Sciences}
\label{sec:gnn-applications}


% TODO for related work on CNNs in Life Sciences, search in Obsidian for
% 'Convolutional Nerual Network' have some nice ones there, including those
% already quite similar to graph convolution

% MAYBE application to Transformers on biological data, c.f. obsidian notes

To the best of our knowledge there is no previous work that connects GNNs to
node duplication.

\citeauthor{tiezzi_GraphNeuralNetworks_2021} provide a method for layouting
graphs using GNNs
\cite{tiezzi_GraphNeuralNetworks_2021}
. As a first approach, they train a GNN to draw graphs based on
ground-truth examples obtained from other graph drawing software. As attributes,
each node is assigned a positional encoding based on the Laplacian Eigenvectors
of the graph. The loss function aims to minimise the distances from the produced
drawing to the ground-truth drawing (modulo affine transformations).
% More
% interestingly, they train a GNN to estimate the probability of intersection of
% any two edges.
% omitting their 'neural aesthetes' here.
Further, inspired by optimisation-based methods such as Stress Majorisation
\cite{gansner_GraphDrawingStress_2005}, they employ a GNN to directly minimise
the stress function on the predicted node coordinates. Note that the
all-pairs-shortest-paths computation has to be done only during training. At
inference time, the model predicts node positions based on the supplied
positional encoding, which is potentially easier to compute. Additional quality
measures, such as the number of edge crossings, can be included in the loss
function without sacrificing the advantage that predictions only ever require
the graph structure and positional encodings and no additional computation.

% introductory blah
% TODO note that GNNs have only recently become more popular
% save to say that popularised since beginnig of 2021? so, not so many
% applications yet? Certainly, very active area, many publications in last year
% or so

% application for node duplication is novel as far as we can see
% application to disease maps is novel



GNNs can generally be applied for three different types of tasks:
\ild{node-level} tasks in which the units of interest are individual nodes,
\ild{edge-level} tasks in which edges are considered as the unit of interest and
\ild{graph-level} tasks in which a statement about the entire graph is sought.

\paragraph{Node-level tasks}
\citeauthor{you_DeepGraphGOGraphNeural_2021}
\cite{you_DeepGraphGOGraphNeural_2021} consider the problem of predicting the
function of a protein based on its sequence. A current challenge is
bioinformatics is the gap between the number of known protein sequences and the
number of protein sequences annotated with a biological function
(\ild{sequence-annotation-gap}). \citeauthor{you_DeepGraphGOGraphNeural_2021}
suggest to consider each protein with respect to other proteins it is known to
interact with. As such, they propose a GNN approach in which proteins are
represented as nodes and connected based on information from Protein-Protein
interaction databases. The function annotations here are in fact Gene Ontology
terms.

Once the biological function of a protein is known, another challenge is to assess the
functional similarity of two proteins. One way to approach this is to compare
the (sets of) Gene Ontology terms of two given proteins. 
\citeauthor{zhong_GO2VecTransformingGO_2020} compute an embedding for GO terms
based on the graph structure of the GO ontology
\cite{zhong_GO2VecTransformingGO_2020}.
These embeddings can be thought
of to encode the term's position in the graph, that is, its relationship to
other terms. A measure of semantic similarity can be derived by comparing the
(sets of) computed embeddings.
% TODO split this off into own section on ontologies/KGs?

\ild{Single-cell RNA-sequencing} (scRNA-seq) is a technology that provides gene
expression data on the level of individual cells. In the work of
\citeauthor{ravindra_disease_2020} \cite{ravindra_disease_2020}, each cell is
represented as a node and features are its gene expression data. Graph
connectivity is defined on a node's $k$ nearest neighbours. The goal is to
predict whether a cell corresponds to a healthy or pathological disease state
w.r.t to Multiple Sclerosis (MS). The motivation is to work towards developing a
diagnostic test for MS based on scRNA-seq technology.
% TODO more on GNNs in scRNA-seq?

% TODO manipur cancer stuff?

% TODO-maybe write about GraphSMOTE in related work
% For oversampling, we can either simply duplicate examples or synthetically
% create new examples. We avoid simply duplicating minority class examples because
% with respect to the classifiers used in this work, we deem this approach
% analogous to supplying balancing class weight coefficients to the classifier.
% When synthetically generating new examples, we of course have to settle on a
% procedure to do so and trust in that it will supply realistic training examples.
% A common technique for tabular data is \textsc{SMOTE}, in which, for a given
% data point $\vec x$, one of its $k$-nearest neighbours $\vec x'$ is selected and
% a new data point is generated by interpolating between $\vec x$ and $\vec x'$,
% i.e. $\vec x_{\text{new}} = \vec x + \lambda (\vec x - \vec x')$  where
% $\lambda$ is a random choice from $[0,1]$. However, note that this does not
% suffice for graph structured data: In addition to attributes, we also have to
% impute network connectivity. While there exist methods to achieve this (e.g. an
% extension of \textsc{SMOTE} to network structured-data), we did not explore this
% approach further due to time constraints and more promising alternatives.

\paragraph{Graph-level tasks} Graph Neural Networks have found highly successful
applications for the problem of molecular property prediction. This is a
graph-level task where the input is a \ild{molecule graph} and we strive to
predict specific chemical properties of this molecule. In a molecule graph,
nodes represent atoms and edges represent bonds. Nodes and edges have attributes
describing e.g. an atoms element, or the type of a bond. Properties to predict
may include toxicity or antibacterial activity. This task is relevant
particularly in the field of drug development where a large number of molecules
has to be tested for potential usefulness as a drug (\ild{virtual screening}).
%
Traditionally, molecules were represented by \ild{fingerprint vectors} which
describe characteristics of the entire molecule such as the presence of
functional groups. While these fingerprints may well serve as input to a neural
network predictor, the structure of these fingerprints is designed manually and
often not directly dependent on the prediction task.
% TODO cite something, maybe smiles or one of
% Mauri et al., 2006; Moriwaki et al., 2018; Rogers and Hahn, 2010
Graph Neural Networks provide the means to compute such fingerprints directly
based on the the structure of the molecule and concrete, low level properties.
Moreover, the extraction and aggregation of input features performed by graph
neural networks is differentiable and thus the entire prediction pipeline can
jointly be optimised end-to-end, yielding task-specific fingerprints.
% TODO make it clearer that fingerprints is another term for this automatic
% feature extraction
\citeauthor{stokes_DeepLearningApproach_2020} use this approach to predict the
growth inhibition against \textit{E.coli}, eventually resulting in the discovery
of experimentally verified potent antibiotics that are structurally different
from known antibiotics \cite{stokes_DeepLearningApproach_2020}.
%
Notably, \citeauthor{duvenaud_convolutional_2015} employ this approach well
before the recent popularisation of Graph Neural Networks in the style of  
\refeq{gnn-framework}.
% TODO just give this a name instead of referring to the eq ('spatial' gnns vs spectral)
Focussed reviews of applications of GNNs for molecular property prediction
\cite{wieder_CompactReviewMolecular_2020}
and drug development in general
\cite{gaudelet_utilising_2020}
can be found in the literature.
%
Further, \citeauthor{baranwal_deep_2020} train a GNN model to compute
fingerprints of molecules that are then used to predict their broad metabolic
pathway class (e.g. carbohydrate metabolism, amino acid metabolism, \etc)
\cite{baranwal_deep_2020}.

% TODO also mention creation of synthetic molecular graphs

% TODO include zuo2021



% TODO elaborate that small-graph/big-data work is different setting
%  (generally much larger number of training samples)


\paragraph{Edge-level tasks} GNNs have been applied for edge-level tasks
particularly for the problem of link prediction in biological interaction
networks. GNNs have been applied to predict interactions between diseases and
drugs \cite{bajaj_GraphConvolutionalNetworks_2017}, interactions between drugs,
proteins and drug side effects \cite{zitnik_modeling_2018} and interactions
between proteins \cite{chereda_ExplainingDecisionsGraph_2021}. Many other
applications can be found in the literature
\cite{zhang_GraphNeuralNetworks_2021}.


\subsection{Other relevant ML approaches}
% particularly talking a bit about metabolic models may be nice?

% ma_UsingDeepLearning_2018
% costello_MachineLearningApproach_2018


\section{Gene Ontology}

\citeauthor{henry_ConvertingDiseaseMaps_2021} extract the relationships such as
reactions, links to phenotypes and genes or relationships to cellular components
given in the \textit{AlzPathway} disease map and construct an ontology of it,
aiming to obtain a more formalised, precise and less ambiguous description of
the contained knowledge \cite{henry_ConvertingDiseaseMaps_2021}.


Work exists to develop similarity measures
between two GO terms, or two sets of GO terms (see \refsec{related-work}).
These are, by definition, pairwise similarity measures. These
could be interpreted as distance measures and these distances could be
considered directly in a machine learning model.

An alternative approach is to find a embeddings (vectors describing feature
representations) of the given GO term s.t. the similarity of their embeddings is
meaningful, i.e. reflects the similarities in the GO graph. In fact, some
approaches for semantic similarity measures depend exactly on such embeddings
\cite{zhong_GO2VecTransformingGO_2020}.

\citeauthor{ostaszewski_ClusteringApproachesVisual_2018} consider the
species-level GO term annotations in \textit{AlzPathway} to automatically
identify semantic clusters in the diagram, adopting a measure of semantic
similarity between GO terms.

% TODO maybe talk about fingerprints aswell (split into subsections)









% ====================================
\chapter{Methods}
\label{sec:methods}


% -------------------------------------
\section{Datasets \& Preprocessing}
\label{sec:datasets}

% TODO mention here that datasets used are described in ch. 5

\subsection{Graph construction}
\label{sec:graph-interpretation}
% how graph is constructed, what considerations one has to make...
Disease maps can be interpreted as bipartite graphs in a natural manner with the
bipartite node sets being the set of interactions and the set of species,
respectively. The disease maps considered in this work are given in an extension
to SBML defined by the \toolname{CellDesigner} tool. The format is very rich in
information and leaves some room for ambiguity concerning graph construction. In
the following, we describe what we take into account to construct a graph.

The most central elements in an SBML model are the lists of reactions and
species aliases (visual representations of species). 
%
An entry in the list of reactions carries references to species taking part in
that reaction. Since different occurences of a species can be visualised as
duplicate species aliases, each entry for a participating species additionally
contains a reference to a specific species alias. Participating species are
distuingished by the role they play in that reaction. Herein, we consider the
basic roles defined by standard SBML: products, modifiers and reactants
\footnote{ \toolname{CellDesigner} provides an even more fine-grained
  distinction of species participating in a reaction. Species can be \ild{main}
  reactants or products, \ild{modifiers} or \ild{additional} reactants or
  products. We omit this for simplicity. See
  \cite{_CellDesignerExtensionTag_2010}, ch. 2.4.
}. We create directed edges for reactants and products in the
direction of the reaction. A modifier is attached by two edges, one in either
direction. For computing structural node features and for message-passing, we
sometimes also consider the
% TODO has this been defined before?
bipartite projection. Herein, we compute the bipartite projection based on the
undirected interpretation of the graph.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\linewidth}
    % TODO diagram illustrating graph construction: combine drawing of SBML
    % with simplified graph structure
    (graph-interpretation) from paper notes
    \caption{Illustration of how a rich SBML model is interpreted as a simple,
      directed graph.}
  \end{subfigure}
  \begin{subfigure}{0.45\linewidth}
    todo: something illustrating CD-SBML structure / distinction between species
    and species aliases -- or not at all? not really super relevant...
  \end{subfigure}
  \label{fig:graph-interpretation}
\end{figure}


A species alias can either be simple or \ild{complex} in the sense that it
represents a container for other species aliases. This is used to represent e.g.
biological complexes of proteins. Simple and complex species aliases are
arranged in an arbitrarily nested hierarchy. For the graph structure, we
consider only top-level elements, that is, we consider complex species aliases
as single nodes and omit their contents \footnote{ Contents of complex species
  aliases will be taken into account when considering GO annotations, see TODO
}. A reaction may involve an entire complex species alias (CSA) or only a
species alias contained in the CSA. Since we interpret the CSA as a single node,
an edge will be attached to it in either case.

Species aliases can also be contained in \ild{compartments} representing
biological cellular compartments or broader notions of spatial relationship. We
do not consider compartments at all herein (see~\refsec{outlook}).

% Attributes such as the species class or the position of the species alias in the
% layout are extracted from the source file and attached as node at

% TODO include diagram of CellDesigner-SBML from miro board?
% ... or anything else illustrating the complexity...



\subsection{Determining ground-truth Labels}
Although numerous disease maps are publicly available, to the best of our
knowledge none are explicitly annotated with a per-alias label indicating node
duplication. In case we are given a sequence of reorganisation steps $(G_1, ...,
G_k)$ , we infer node labels by comparing successive steps $G_t$ and $G_{t+1}$.
In
% TODO % define reorganisation steps
case we are given only a single disease map $G$, we first construct a collapsed
version $G_0$ by collapsing any species aliases
% TODO calling this G is fishy
corresponding to the same species into a representative node and moving any edges
incident to aliases to the corresponding representative. We then proceed by
comparing $G_0$ and $G$ like reorganisation steps.
%
In order to make our results comparable to the work of
\citeauthor{nielsen_MachineLearningSupport_2019}
\cite{nielsen_MachineLearningSupport_2019}, we employ the same algorithm for
inferring ground-truth labels. Because the algorithm has received little
explicit treatment in the original publication, we motivate and describe it here
in detail for clarity.
%
% % We motivate the algorithm by first outlining requirements to a procedure for
% % detecing duplicated aliases.
% \begin{itemize}
% \item The case in which merely a copy of a node is introduced and edges are
%   re-attached should also be captured. 
%   % algorithm adresses this by fact that copy will find its duplication parent
%   % (carried over parent will not)
% \item A subgraph reorganisation may include a valid duplication even if an
%   adjacent edge has been removed
%   % adressed by considering to-from/from-to neighbours instead of covering
%   % (ex-remove-edge)
% \end{itemize}
% % TODO does not make much sense to include this list if there are only two
% % points. Its basically saying the same thing as in the sentence above.

A simple approach that comes to mind is to consider nodes newly introduced in
$G_{t+1}$ and look for a subset $W \subset V(G_{t+1})$ whose neighbourhood
completely covers the neighbourhood of some node in $G_{t}$, i.e. $\bigcup_{w
  \in W} \mathcal{N}_{t+1}(w) = \mathcal{N}_t(v)$ for some $v \in G_{t+1}$.
However, this does not suffice. It is important to note that we can make no
assumptions about what manipulations were made to create $G_{t+1}$ from $G_t$.
In particular, nodes may have been removed, added or duplicated and edges may
have been added, removed or re-wired.
%
To deal with this, instead of finding the duplicates of a given node in
$G_{t}$, the algorithm seeks to identify a possible \ild{duplication parent} of
a given node in $G_{t+1}$. The basic idea is that the neighbourhood of
duplicates will be at least partially included in the neighbourhood of the
duplication parent.

Intuitively, the algorithm works by starting at a given node $v_i \in
V(G_{t+1})$, identifying its neighbour nodes in $G_{t+1}$ and considering their
neighbours in $G_t$. If $v_i$ is a duplicate, we expect its duplication parent
to be among these nodes.
% TODO mention that the alg also captures cases where a copy is introduced and
% existing node is kept (in that case the parent will be identified when
% considering the newly introduced node.)

\begin{algorithm}[h]
  \DontPrintSemicolon
  \label{alg:identify-duplicates}
  \caption{ Procedure to identify duplication parents. Transcribed from
    \citeauthor{nielsen_MachineLearningSupport_2019} \cite{nielsen_MachineLearningSupport_2019}.
  }
  \setstretch{1.2} \KwData{Directed graphs $G_t$ and $G_{t+1}$ (reorganisation
    step), node $v_i \in G_{t+1}$} \KwResult{Duplication parent of $v_i$ or
    None} $W_+ \gets \neighb_t^- ( \neighb_{t+1}^+(v_i) )$ \; $P_+ \gets
  \neighb_{t+1}^- ( \neighb_{t+1}^+(v_i) ) \backslash~ W_+$ \; $W_- \gets
  \neighb_t^+ ( \neighb_{t+1}^-(v_i) ) $ \; $P_- \gets \neighb_{t+1}^+ (
  \neighb_{t+1}^-(v_i) ) ~\backslash~ W_- $\;
    %
    \eIf{$W_+ = \emptyset \lor W_- = \emptyset$}{
      $P \gets P_+ \cup P_-$
      \label{line:ambig-cup}
      \;
    }{
      $P \gets P_+ \cap P_-$
      \label{line:ambig-cap}
      \;
    } 
    \eIf{
      $\ensuremath{\norm{P}} = 1$
      \label{line:p-1-check}
    }{
      Let $w$ be the single element in $P$ \;
      \Return $w$ \;
    }{
      \Return None
    }
\end{algorithm}
% TODO what kind of ID? alias ID?
% TODO terminology aliases vs graph nodes

The procedure is given in pseudocode in \ref{alg:identify-duplicates}. For
directed graphs, let the \ild{positive neighbourhood} of $v$ in $G_k$ $\neighb_k^+(v)$ be
given as $\{w ~|~ (v,w) \in E(G_{k})\}$ an the \ild{negative neighbourhood}
$\neighb_k^-(v)$ as $\{w ~|~ (w,v) \in E(G_k)\}$.
The set operations identify nodes solely based on their alias ID. This means that
$V(G_t)$ and $V(G_{t+1})$ may potentially have nonempty intersection (indeed,
the algorithm relies on it).

Line \ref{line:ambig-cap} states that a valid duplication parent must be
reachable from both positive and negative direction. This is only relevant if
there is no positive (resp. negative) neighbourhood shared between the
reorganisation steps (which is also the case if the target node is a sink, 
resp. source). Line \ref{line:ambig-cup} takes this into account. Then, a
duplication parent may still be uniquely identified if there is a single shared
neighbour in the opposite direction (see \reffig{fig:ex-no-neighb}).
Line \ref{line:p-1-check} handles cases when multiple candidate duplication
parents exist and we cannot infer a unique single one (see example \reffig{fig:ex-p-1}).
The algorithm is able to identify duplication parents even if edges have been
removed (see example \reffig{fig:ex-remove-edge}).

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.23\textwidth}
    subfig
    \caption{Example in which there is no shared positive neighbourhood.
      However, a unique duplication parent can still be identified because node
      2 has unit out-degree.}
    \label{fig:ex-no-neighb}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    subfig
    \caption{Example where a unique duplication parent cannot be identified
      due to ambiguity.}
    \label{fig:ex-p-1}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    subfig
    \caption{Parents can still be inferred if edges are removed. Without node 7,
    however, the case would be ambiguous}
    \label{fig:ex-remove-edge}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    subfig
    \caption{Ambiguities are resolved by considering both positive and negative neighbourhoods.}
    \label{fig:ex-ambig1}
  \end{subfigure}
  \caption{Examples for Algorithm \ref{alg:identify-duplicates}}
  \label{fig:alg-examples}
\end{figure}

\subsection{Feature Engineering}
\label{sec:feature-selection}

% TODO Describe feature sets used, how features are calculated...

% General remarks on possible features
In general, there are three aspects of a disease maps based on which features
can be defined. The first is the \ild{structural} aspect in which we use
graph-theoretical measures to characterise species aliases (graph nodes) based
on their connectivity in the network. This was considered in detail in the
feature engineering step in the work of
\citeauthor{nielsen_MachineLearningSupport_2019} and was used in numerous
approaches for metabolic networks (see \refsec{related-work}).

The second aspect is \ild{semantic}: species are annotated with biological
domain knowledge, commonly in the form of links to databases that provide
additional information about that species (see \nameref{sec:background}). In
this work, we aim to explore how to exploit Gene Ontology term annotations.

The third aspect is that of \ild{layout}: Prior to making a prediction on node
duplication, the disease map may already have been laid out to some extent.
If so, positions of species aliases certainly carry some meaning. Which exactly,
however, is unclear since positions are most likely determined by a mixture of
layout requirements, semantic arrangement and preference of the curator.

For the scope of this work, we avoid using layout-based predictors
% TODO mention term predictor in background
since we only have one practical instance in which we have layout information
and ground-truth labels available, which is when making a prediction for a
\dataname{AlzPathway} reorganisation step and comparing it to the actual next
step in the reorganisation sequence. However, this is the only disease map for
which reorganisation steps are available and it is of limited size. Thus we
decide to omit this approach for now, while acknowledging that considering
layout information may lead to interesting approaches (see
\nameref{sec:future-work}) \footnote{ We do consider layout information as a
  criterion for attaching edges to duplicates, see \refsec{edge attachment}. }.




\paragraph{Structural Features} The following structural features were defined
and used by \citeauthor{nielsen_MachineLearningSupport_2019} and we re-implement
them for comparability. Let $\sigma_{st}(v)$ denote the number of shortest paths
from $s$ to $t$ passing through $v$ and $\sigma_{st}$ is the number of shortest
paths from $s$ to $t$. Since in the following, we always consider the graph to
be undirected, $\sigma$ is symmetric w.r.t $s, t$. 

\begin{itemize}
\item \featname{degree}: The degree of a node, counting both incoming and
  outgoing edges.
\item \featname{clustering_coefficient}: The \ild{clustering coefficient}
  \cite{brandes_NetworkAnalysisMethodological_2005}
  for a
  node $v$ is given as
  \begin{align*}
    \frac{2 \tau(v)}{\deg(v)(\deg(v)-1)}
  \end{align*}
  where $\tau(v)$ is the number of possible triangles through $v$ in the graph.
\item \featname{betweenness_centrality}: The \ild{betweenness centrality} of $v$
  reflects the number of shortest paths that pass through $v$:
  \begin{align*}
    & \sum_{s \not= v} \sum_{t \not= v} \frac{\sigma_{st}(v)}{\sigma_{st}}
  \end{align*}
\item \featname{closeness_centrality}: As used here, the \ild{closeness
    centrality} for $v$ is given by
  % There are different ways to define a
  % closeness-based centrality measure. The definition used here is:
  \begin{align*}
    \frac{\norm{V}-1}{\sum_{s \not= v \in V} d(s,v)}
  \end{align*}
  where $d(s,v)$ is the shortest-path distance from $s$ to $v$.
\item \featname{eigenvector_centrality}: The \ild{eigenvector} centrality is a
  measure of transitive importance of a node. The basic idea is that a node is
  important if it is linked to other important nodes. The centrality values of
  each node are given by the components of the principal eigenvector of the graph's
  adjacency matrix.
\end{itemize}

Additionally, in order to capture the characteristics of a node's direct
neighbourhood, the following secondary features are computed:
\begin{itemize}
\item \featname{neighbour_centrality_statistics}: For node $v$, this the stacked
  vector of statistics over several centrality measures of neighbours of $v$.
  The statistics are mean, minimum, maximum and standard deviation. The
  centralities are betweenness, closeness and eigenvector centrality and degree.
\item \featname{distance_set_sizes}: A vector of length $k$ in which the $k$-th
  entry is the normalised number of nodes exactly $k$ hops from $v$. Here, $k=5$
  is considered. The values are normalised by the number of nodes reachable in a
  grid graph via $k$ hops.
\end{itemize}

Except for the clustering coefficient, additionally all characteristics are also
computed on the bipartite projection and included as separate features.
Except for \featname{distance_set_sizes}, all features are min-max-normalised
w.r.t all given training graphs.

% construction of bipartite projection is based on undirected graph
% centralities are based on undirected graph

Additionally, we consider the directed degrees (\featname{in_degree},
\featname{out_degree}). This is motivated by the notion that a species alias may
have a different biological meaning w.r.t node duplication if either in- our
% TODO biological good word to use here?
out-degree is higher, both are balanced or the node is a source or a sink.
% TODO this is repeating to what we write in the results, where to leave it?
% generally think the distance may be bit large and this section has a too fuzzy
% distinction between nielsen features and our thoughts
% TODO this is repeating to what we write in the results, where to leave it?
% generally think the distance may be bit large and this section has a too fuzzy
% distinction between nielsen features and our thoughts

Because the input networks are of nontrivial size (see \refsec{datasets-used}),
computation time has to be taken into account.
% 
Particularly attributes relying on global information (such as e.g. closeness
centrality) can provide a challenge for high-level graph libraries such as
\toolname{networkx} and may not complete in a reasonable amount of time. More
sophisticated implementations working with a lower-level language such as those
provided by the \toolname{igraph} library used herein show a significant speedup.
% TODO cite both -- by website?
\citeauthor{nielsen_MachineLearningSupport_2019} additionally consider the
centralities of a given node in its ego graphs of sizes $3$ and $5$. Due to time
constraints, we omit this feature. We note that we achieve comparable
classification performance as reported the original work even without using that
feature (see \nameref{sec:experiments-results}).



\paragraph{Semantic Features}

A simple semantic feature that \citeauthor{nielsen_MachineLearningSupport_2019}
propose is the one-hot-encoding the species type.
% TODO maybe define one-hot-encoding if we can think of how to do it simply
Possible species types are
\textit{protein}, \textit{RNA}, \textit{simple molecule}, \textit{ion},
\textit{gene}, \textit{phenotype}, \textit{drug}, \textit{complex},
\textit{degraded} and \textit{unknown}. 


In addition, we explore using Gene Ontology term annotations to infer meaningful
features.
%
We hypothesize that, in general, a node should be duplicated if its
neighbourhood is highly heterogeneous, i.e. the species alias establishes false
connectivity between actually unrelated processes.
Previous approaches commonly focus on structural or layout-based features.
However, these facets involve domain knowledge only implicitly, if even.
As a characterisation of neighbourhood homogeneity, we seek
to explicitly include domain knowledge about what biological processes a species
is involved in. To this end, we consider the Gene Ontology terms linked to each
species in the given disease map. In particular, we consider those terms of the
``\textit{biological process}'' subtree of the GO graph (see
\refsec{background}).


We extract GO annotations directly from the given \toolname{CellDesigner}-SBML
files.
% TODO how, for different datasets?
% ids are mapped via that api service
% GO graph obtained from website (date)

Given a set of GO term annotations for a species alias, we have to derive a
fixed-length numeric vector from them that can be used as a feature vector.
%
We achieve this by computing an \ild{embedding} vector for each GO term that
reflects its position in the GO ontology graph and thus its semantics. We
acknowledge that there is previous work developing pairwise similarity measures
between GO terms (see \nameref{sec:related-work}). However, the direct,
embedding-based approach has the following advantages:
%
\begin{itemize}
\item We are alleviated of the choice of an additional similarity measure
  (effectively leaving that problem to the classifier). This also means that,
  particularly in case of a GNN classifier, the neural network may potentially
  be able to capture more flexible relationships than what we would encode with
  some similarity measure.
\item This gives us a simple approach for including annotation information for
  complex species aliases by combining the embeddings of the contained nodes.
\item Since in the end we aim to assess the heterogeneity of a node's
  neighbourhood, the consideration of embedding values could be incorporated in
  the message-passing step of a neural network (see \nameref{sec:outlook}). Further,
  node-level information may be useful for the task of finding an attachment of
  edges after duplication.
\end{itemize}

% TODO describe node2vec in background?
For computing an embedding vector for a given GO term, we closely follow the
approach of \citeauthor{zhong_GO2VecTransformingGO_2020}
\cite{zhong_GO2VecTransformingGO_2020} and encode a term's position in the GO
graph via the \toolname{node2vec} algorithm.
% TODO n2v hyperparams based on that pape and then adjusted some?

The above considerations lead to the following concrete feature definitions:
\begin{itemize}
\item \featname{GO_embedding}: The basic idea of this approach is to only provide an
  encoding of the term's position in the GO graph as node feature. A GNN model
  could then potentially capture characteristics of a target node's
  neighbourhood via its aggregation function.
  % TODO move sentences around, we are basically already describing all of this above
\item \featname{GO_stddev}: We additionally try capturing neighbourhood
  heterogeneity by interpreting the embeddings as points in an euclidean space
  and deriving a measure for the spread of these points around their centroid
  (mean). The idea is inspired by the measure of standard deviation, i.e. the
  average squared distance from the mean. This can conventienly be expressed as
  the sum of variances over each dimension. Let $(\vec x_1, ..., \vec x_n)$ be
  the embeddings of neighbour nodes, and let $\bar{\vec x}$ be the centroid
  (i.e. dimension-wise mean). Assume the points lie in a $D$-dimensional
  euclidean space. Let $(\vec x_i)_k$ denote the $k$-th entry of $\vec x_i$. We
  then consider $\sigma = \sqrt{\sigma^2}$ as a measure of spread, given by
  \begin{align*}
    \sigma^2 &= \nicefrac{1}{n} \sum_{i=1}^n \norm{\bar{\vec x} - \vec x_i}_2^2 \\
             &= \sum_{d=1}^D \nicefrac{1}{n} \sum_{i=1}^n
               (\bar{\vec x})_d - (\bar{\vec x_i})_d)^2 \\
             &= \sum_{d=1}^D \text{Var}(\left[
               (\vec x_1)_d, ..., (\vec x_n)_d
               \right])
  \end{align*}
  where $\text{Var}(\left[(\vec x_1)_d, ..., (\vec x_n)_d \right])$ is the
  variance along dimension $d$.
  Note that this approach encodes characteristics of the embeddings across a
  node's neighbourhood directly into a single feature vector and can thus be
  used with any classifier, such as SVMs.
\end{itemize}


There are two cases when we are required to
aggregate a set of embeddings into a single value. On the one hand, a single
protein can be annotated with several GO terms. On the other hand, a complex
species alias potentially contains muliple annotated aliases. In both cases,
we take the mean.


% - compute embedding for each GO term* via node2vec in the GO graph
% note: embedding could also have been computed with GNN method (node
% attributes? positional encoding?) or even integrated in same differentiable
% pipeline. -- but stick to simplicity, has the advantage that embs can be precomputed
% ...- use these as features directly, for complexes take average over all
% contained (GO embedding)
% ...- compute stddev over neighbourhood to directly capture 'diversity'
% * mentioned in the DM




\section{Training \& Classification}
\label{sec:classification}

% TODO would expect info on choice of loss function, training procedure etc here?

The basic pipeline is illustrated in \reffig{diag-pipeline}. The general
approach is to extract data from the \toolname{CellDesigner}-SBML files
describing some disease maps, and construct for each an attributed, directed,
bipartite graph $G$. Additionally, we infer ground-truth labels for nodes in
$G$. We aim to classify nodes in $G$. For each node, we compute a feature
representation. For the SVM model, the feature vector and ground-truth label is
the only input. For GNN models, additionally the network structure is given.

Nodes corresponding to complex species aliases and nodes of degree less than two
are excluded from prediction. This means they will not be considered as input
examples when training or evaluating the classifier. Note that these nodes are
still part of $G$ and potentially influence the features of other nodes.
Further, excluded nodes will still participate in the message-passing steps of
GNN models.
% TODO cite to previous secs

The set of thus constructed input graphs is partitioned into training and
testing graphs. For the concrete choice of training and testing partitions, we
explore different settings, see \nameref{sec:experiments-results}.

\begin{figure}[h]
  \centering
  (diag-pipeline) on paper
  % TODO diagram: SBML entitiy → initial feature representation → intermediate
  % representation through ML model → classifier
  % additional branch SBML -> network structure
  % somewhere show that we are exlucing some from classif but still using them for MP?
  % (diag-pipeline)
  \caption{Basic pipeline}
  \label{fig:diag-pipeline}
\end{figure}




% ML setup: what is train data, what is test/validate data


% TODO not sure if this should go into Methods
The task at hand comes with a particular set of challenges:
\begin{itemize}
\item Different disease maps potentially have very different characteristics
  (see e.g. the difference between \ADMap or \PDMap and \ReconMap in
  \reffig{fig:maps-summary}), which may make it difficult for the model to
  generalise to unseen disease maps. We evaluate generalisation ability
  throughout our experiments.
\item Typically, only a few nodes are duplicated (see \reffig{fig:maps-summary}).
  Thus, the positive class is underrepresented in the dataset. We have to ensure that
  model performance does not suffer due to class imbalance. We address this
  issue by considering two approaches: First, we undersample the majority class.
  Second, we provide class-specific weights to the ML models such that an
  example of the minority class will be assigned more importance.
  % \item  choice of predictors is not clear?
\item The ground-truth labels may not be perfectly reliable.
  For one, the decision whether a node was duplicated or not during curation is
  subjective to the expertise and preference of the curator. The criteria that
  were relevant to the curator are most likely not perfectly reflected in the
  features we provide the model with.
  % (NOTE which is the entire motivation for a ML
  % approach with very complex models).
  Thus, it may be the case that some
  training examples are contradictory in label \wrt their feature
  representation. Further, considering several reorganisation steps is likely to
  introduce contradictory examples if a node is duplicated in some step but not
  in an earlier one.
\item The number of datapoints used for training and evaluation is relatively
  low compared to other common use-cases for Machine Learning. Care has to be
  taken to avoid overfitting, i.e. the model adjusting overly well to the training
  data at the cost of prediction performance on validation data not seen during
  training. Further, particularly when partitioning the available data into
  subsets for training and testing, we have to make sure we still have plenty of
  representative examples in each subset. We address this issue in part by avoiding to
  split a disease map internally into training and testing subsets. Further, we
  want to highlight that SVMs an GNNs have been applied successfully on datasets
  of comparable size.
  % TODO cite things
  Note also that the GNN architectures we considere herein are of
  much smaller complexity (in terms of number of model paramaters) than famous
  neural network architectures used in Computer Vision or Natural Language
  Processing
  % TODO cite
  and may thus not require a particularly large amount of training
  data to fit all parameters.
\end{itemize}

Experiments and results are given on \refsec{experiments-results}.


\section{Attachment of edges}
\label{sec:edge attachment}

% TODO make it clear that we are assuming a given layout

Assume we are given a binary classifier that decides whether a node should be
duplicated. If a node $v$ is eligible for duplication, we next need to determine
how many duplicates to introduce and how to distribute edges to and from $v$
across the duplicates. Formally, we aim to find a partition of the neighbourhood
of $v$. Based on the intuition that a good duplication is one that reduces the
heterogeneity of the neighbourhood, we can characterise the partitions as
\ild{clusters} in the sense that intra-cluster distances are smaller than
inter-cluster distances.

The choice of distance metric is open. Of particular interest are metrics that
reflect semantic similarity of attached GO terms \footnote{ Indeed,
  \citeauthor{ostaszewski_ClusteringApproachesVisual_2018} applied GO semantic
  similarity to cluster nodes in a disease map. }. However, because in the given
datasets, most aliases are annotated with a relatively large number of GO terms
and exploiting these annotations for classification was problematic (see \nameref{sec:experiments-results}),
% did not yield gains in
% classification performance most likely due to high ambiguity,
we opt for a
simpler approach first and leave this open to future work. % TODO mention there.
Instead, we consider the layout positions of aliases and their euclidean
distances. Note that this approach is only applicable if layout information is
actually given, i.e. this approach can not be used if we construct a collapsed
graph from a single given disease map (unless we would infer layout information
for the newly constructed, collapsed graph).

Particularly when considering euclidean distances in the layout, a suitable
clustering algorithm needs to handle outliers in a sensible manner.
Additionally, we do not know the number of clusters in advance. This eliminates
basic partition-based methods such as $k$-\textsc{Means} and extensions.
%
Further, we seek to assign all points to a cluster, i.e. we do not want to
exclude any points as noise. Also, since different node neighbourhoods have
potentially different scales, we aim to avoid having to specify hyperparameters
like distance thresholds as they are used in density-based clustering algorithms
like \textsc{DBSCAN} or \textsc{Optics}.
%
A family of clustering algorithms that seems well suited is that of
\ild{agglomerative} clustering: Initially, each point is assigned its own
cluster. Iteratively, the two clusters with the smallest inter-cluster-distance
are merged until only a single cluster remains. This yiels a hierarchical
clustering tree, also called \ild{dendrogram}, as depicted in
\reffig{neighb-clust-examples}.

There are several canonical choices of distance measures between two clusters $C_1$ and
$C_2$. The most simple ones employed for agglomerative clustering are:
\begin{align*}
  \text{\ild{single linkage:}} &~~ d(C_1, C_2) = \min_{p \in C_1, q \in C_2} d(p,q) \\
  \text{\ild{complete linkage:}} &~~ d(C_1, C_2) = \max_{p \in C_1, q \in C_2} d(p,q) \\
  \text{\ild{centroid linkage:}} &~~ d(C_1, C_2) = d(\text{mean}(C_1),\text{mean}(C_2))
\end{align*}
where $C_1$ and $C_2$ are considered to be sets of points. Complete linkage and
centroid linkage seem inadequate since they would be strongly affected by large
in-cluster variances and do not work well if clusters are not convex.
% and do not work well if clusters not convex

Setting a threshold value on the maximum dissimilarity inside a cluster, we
obtain a concrete clustering. This can be thought of as ``cutting off'' the
dendrogram at a specific height. Note that here we do not have to specify the
number of clusters but instead a threshold dissimilarity. This threshold can be
determined automatically via a heuristic:
% \footnote{
%   This is basically the \ild{elbow method} in which we plot clustering quality
%   % TODO citation?
%   against number of clusters and aim to find an elbow (i.e. bend) in the curve.
%   Clustering quality here would be the minimum inter-cluster distance of two yet
%   unmerged clusters.
% }
We look for the strongest increase in the distance to
the next closest cluster before each merge step. Formally, let $d = (d_1, ..., d_k)$
be the monotonically increasing sequence of inter-cluster distances at which a
merge occured. The first discrete derivative $d'$ of this sequence gives the step
sizes while the second derivative $d''$ describes the change rate in step sizes. The
index of the maximum in $d''$ yields the number of clusters.
Note that we require $k \geq 8$ points for determining at least two values in the second
discrete derivative. In case of $k < 8$, we fall back to the first derivative
(step size). Examples show that this is a good approximation for a low number of points.
%
As special cases, since the decision to duplicate is assumed to be already given
by the classifier, we exclude the possibility of returning only a single
cluster. If $\norm{\neighb(v)}=2$, then we always trivially split. 

Note that thus we characterise the procedure to identify the number of clusters
independently of the concrete scale of the data. However,
we can see that this procedure strugges if
intra-cluster variances are diverse an the internal variance of a
cluster is close to another inter-cluster distance (see \reffig{neighb-clust-examples}).
We accept this disadvantage
for now and hypothesize it has little practical impact in this use-case.



\section{Implementation}

% features are cached...

% learning on graphs, GNN:
% PyTorch, PyTorch Geometric, GraphGym, Extensions


% SVM: scikit-learn

% 



% ====================================
\chapter{Experiments \& Results}
\label{sec:experiments-results}

% TODO section overview


\section{Datasets used for training and evaluation}
\label{sec:datasets-used}

A visual overview of the disease maps considered in this work is provided in
\reffig{fig:maps-summary}.

The \ild{AlzPathway} map describes signaling pathways related to Alzheimer's
Disease. Since its initial publication
\cite{mizuno_AlzPathwayComprehensiveMap_2012}, it has received several updates
and further analyses \cite{ogishima_MapAlzheimerDiseasesignaling_2013,
  ogishima_AlzPathwayUpdatedMap_2016, mizuno_NetworkAnalysisComprehensive_2016}.
%
The map has received additional curation focussing on increasing readability by
means of reorganising existing network elements
\cite{ostaszewski_AlzPathwayRegorganisationSteps_2021}.
This includes the duplication
of some species aliases. During curation, snapshots of intermediate progress
(\ild{reorganisation steps}) were saved. Note that the reorganisation steps are
not atomic: each step includes modifications to several nodes and edges.
This sequence of reorganisation steps served as the basis for the
work of \citeauthor{nielsen_MachineLearningSupport_2019} to train an SVM
classifier to predict node duplication
\cite{nielsen_MachineLearningSupport_2019}. In this work, we consider these
reorganisaton steps for training data (\dataname{AlzPathwayReorg}).
Note that from the sequence of reorganisation steps, we exclude the steps that
do not correspond to any duplication events.
Further, we consider
the last of the reorganisation steps, i.e. the final result as a single,
independent map (\dataname{AlzPathwayReorgLast}).

The \ild{PDMap} \cite{fujita_IntegratingPathwaysParkinson_2014} describes the
major pathways involved in the pathogenesis of Parkinson's Disease. The map can
be explored via a hosted \toolname{Minerva} instance, reachable at
\url{https://pdmap.uni.lu/minerva}. For this work, we consider the version of
this map as exported from \toolname{Minerva} at 2021-09-07.

\ild{ReconMap}
\cite{noronha_ReconMapInteractiveVisualization_2017}
is a visual representation of the genome-scale metabolic model
\ild{Recon 2} \cite{thiele_CommunitydrivenGlobalReconstruction_2013} that aims
to comprehensively model the human metabolism. It can be viewed at \url{https://vmh.life/minerva}.
% TODO what version from when?

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.32\textwidth}
    % ADReorgLast summary
    \includegraphics[width=\linewidth]{generated/AlzPathwayReorgLast.png}
  \end{subfigure} 
  \begin{subfigure}{0.32\textwidth}
    % PDMap summary
    \includegraphics[width=\linewidth]{generated/PDMap19.png}
  \end{subfigure} 
  \begin{subfigure}{0.32\textwidth}
    % ReconMap summary
    \includegraphics[width=\linewidth]{generated/ReconMapOlder.png}
  \end{subfigure} 
  \caption[
  An overview of characteristics of networks used for training.
  ]{ An overview of characteristics of the \textit{collapsed} networks
    used for training. The count of species aliases and reactions is effectively
    the count of the bipartite node sets in the constructed graphs. Since we are
    consiering collapsed diagrams, the count of species aliases equals the count
    of species.
    Networks and labels are determined as described in
    \refsec{datasets}. }
  \label{fig:maps-summary}
\end{figure}
% TODO consistent ordering of bars!
% TODO assess&remove&mention-explicitly node degree outliers for ReconMapOlder
% to make plot more informative


% TODO give table or overview or smth of how many data points we will have netto
% for classification
% if these are only these 4 we could really create one nice image/plot for each.


% TODO mention that stuff like feature computations and other graph
% representations are cached?




% TODO motivate reporting ROC curves, cutoffs etc
% TODO do numerical scores beyond AUC make sense?

% TODO mention discrepancy between sequence and collapsed

% TODO using sequence of train and collapsed as validate may be problematic
% because different characteristics?
% particularly reconmap looks quite different?
% cf train-on-many?

% TODO adress concern 'but is it OK to use NN on small dataset'
% by providing examples of comparable work
% by saying that GNNs are potentially of much lower complexity than the computer
% vision NNs you see.

% TODO practical concerns on SVMs:
% - few hyperparams
% - formulation naturally seems suited for little training data
% maybe notes from end of here: https://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture6.pdf


% TODO practical concerns on NNs:
% many hyperparameters that need to be picked/tuned
% convergence not guaranteed
% unique 'solution' not guaranteed
% complex / expensive


\section{Basic hyperparameter search}
% cf [[tune or pick hyperparams]]

The models were trained on the AlzPathway reorganisation steps (\ADMap)
and evaluated on the Parkinson's Disease Map (\PDMap). We select the
best-performing model \wrt AUC score.

\subsection{Support Vector Machine}
% - svm-repro
% - train-on-many/train-on-many-svm

For the choice of kernel function, we pick the RBF kernel (see
\refeq{rbf-kernel}) as a heuristic choice since
\citeauthor{nielsen_MachineLearningSupport_2019} showed that a generally
best-performing kernel cannot easily be determined and showed that the RBF
kernel generally achieves good performance.
%
Since we use a different SVM implementation than \nielsen, that might
potentially interpret the parameters slightly differently, for the choice of
\cd{C} and $\gamma$, we additionally perform grid search over the same ranges
of values.
%
The results are given in
\reftab{tab:svm-hyperparams}.

\begin{table}[h]
  \begin{tabular}[h]{| l | l | l |}
    \textit{Hyperparameter} & \textit{Searched range} & \textit{Choice} \\
    \hline
    Cost (C) & $2^{-9}$ to $2^3$ & 0.5 \\
    Gamma ($\gamma$) & $2^{-3}$ to $2^{9}$ & 0.1
  \end{tabular}
  \caption{Considered SVM hyperparameters, value range searched via grid search and
    best identified combination of values.}
  \label{tab:svm-hyperparams}
\end{table}

% TODO do the hyperparams agree with previous work?


\subsection{Graph Neural Network}
% see experiment dirs:
% - gcn-projection was main hyperparam search
% - small-optimisations

The range of searched hyperparameters is based on the work of
\citeauthor{you_design_2020} who systematically evaluated choices of hyperparameters for
various tasks \cite{you_design_2020}. They suggest a constrained design space
for Graph Neural Networks for tasks such as node classification.  

\begin{table}[h]
  \begin{tabular}[h]{| l | l | l |}
    \textit{Hyperparameter} & \textit{Searched range} & \textit{Choice}  \\
    \hline
    Dropout & \cd{[0.0, 0.1,0.2,0.4]} & \cd{0.0} \\
    Aggregation function & \cd{[add, mean, max]} & \cd{add} \\
    Fully-connected layers before message-passing & \cd{[1,2]} & \cd{2}\\
    Fully-connected layers after message-passing & \cd{[2,3]} & \cd{2}\\
    Message-passing layers & \cd{[2,4,6,8]} & \cd{2} \\
    Connectivity & \cd{[skip_sum, skip_cat]} & \cd{skip_sum} \\
    Activation function $\sigma$ & \cd{[PReLU]} &   \\
    Use batch normalisation? & \cd{[yes]} & \\
    Learning rate $\eta$ & \cd{[0.01]} & \\
    % TODO Learning rate decay & \cd{[0.0, *0.1*]} \\
    % TODO weight decay, normalise adj, cf small-optimisations
    Optimiser & \cd{[adam]} & 
  \end{tabular}
  \caption{Considered hyperparameters for the GNN models. In case there are
    multiple possible values, the best hyperparameter combination is given in
    the third column.} 
  \label{tab:gnn-hyperparams}
\end{table}

% TODO elaborate on hidden layer size

% TODO explain dropout, skipsum


\section{Reproducing previous work \& Comparison to GNN}
% svm repro, svm-repro-reconmapolder

In this section, we reproduce the original task from \nielsen using an SVM
classifier. Additionally, we introduce a simple GNN classifier that operates on
the same input data as the SVM, using the bipartite projection of the disease
map graph for message-passing (see \refsec{graph-interpretation}). Further
variants and extensions of the GNN model will be evaluated in the following
sections.

% TODO describe what features we really use here

The models were trained on the AlzPathway reorganisation steps (\ADMap)
and evaluated on the Parkinson's Disease Map (\PDMap) and on \textit{ReconMap}.

% NOTE taking graphic from repeats experiment here
\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro/results/config-svm/roc.png}
    \caption{SVM classifier.}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-repeats/results/config-gnn/roc.png}
    \caption{GNN classifier, 5 repeats.  }
  \end{subfigure}
  \caption{ROC Curves for SVM and GNN classifiers trained on \ADMap
    and evaluated on the same dataset (\textit{training set}, dashed line) or on \PDMap
    (\textit{testing set}, solid line).}
  \label{fig:svm-repro-comparison}
\end{figure}
%
\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-repeats/results/comparison/roc.png}
    \caption{Comparison of ROC Curves of SVM and GNN classifiers.}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-repeats/results/comparison/cutoffs.png}
    % TODO mention that this is randomly chosen repeat
    \caption{Comparison of \FPR values at specific \TPR cutoff values (lower is better). These
      values are specific points from the ROC curve.}
  \end{subfigure}
  \caption{Direct comparison of SVM and GNN classifier. The data
    corresponds to the solid lines in \reffig{svm-repro-comparison}.}
  \label{fig:svm-repro-roc-train-test}
\end{figure}
% TODO coalesce into single figure s.t. they dont split?

% TODO discussion on ROC curves and on cutoffs
% - In general, models able to generalise
% - SVMs and GNNs pretty much the same for high precision, GNNs slightly better
% in the middle
% on cutoffs: compare to what is given in nielsen

\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro/results/config-gnn/loss.png}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro/results/config-gnn/loss-25.png}
  \end{subfigure}
  \caption{Total loss value at each training epoch of the GNN model. The right-hand-side plot a
    focussed view on the same data.}
  \label{fig:svm-repro-loss}
\end{figure}
% TODO discussion on loss value -- maybe actually put this in discussion, since
% this is a thing for all experiments/tasks?
% TODO also show performance on train
% -- will show that already sort of hard to overfit on training data?

% TODO make plots less high? could save some space there...
% (not a good idea for ROC plots because there width vs height ratio is
% meaningful for visual interpretation)

% TODO mention somewhere that we do not do batched training

% TODO mention that this is an advantage of NNs since it seems to be able to
% handle messy data better?



% NOTE this is for ReconMap
\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-reconmapolder-repeats/results/comparison/roc.png}
    \caption{Comparison of ROC Curves of SVM and GNN classifiers.}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-reconmapolder-repeats/results/comparison/cutoffs.png}
    \caption{Comparison of \FPR values at specific \TPR cutoff values (lower is better). These
      values are specific points from the ROC curve.}
  \end{subfigure}
  \caption{(\ADMap $\rightarrow$ \ReconMap).
    Direct comparison of SVM and GNN classifier evaluated on \ReconMap. The data
    corresponds to the solid lines in \reffig{svm-repro-comparison}.}
  \label{fig:svm-repro-reconmapolder-roc-train-test}
\end{figure}
% NOTE this is for ReconMap
\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-reconmapolder/results/config-gnn/loss.png}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-reconmapolder/results/config-gnn/loss-25.png}
  \end{subfigure}
  \caption{Total loss value at each training epoch of the GNN model, evaluted on
    \ReconMap. The right-hand-side plot a focussed view on the same data.}
  \label{fig:svm-repro-reconmapolder-loss}
\end{figure}

To avoid actually overfitting, for overall performance evaluation, we consider
the model state at the training epoch that performs best on the validation
split. This is not necessarily the best epoch \wrt the training split, see
\reffig{fig:svm-repro-loss}. The tradeoff can also be seen in shape of imperfect
performance on the training split in the ROC curves.
% 
% TODO terminology: split
%
Note that, in practise, a ML model should be able to reliably generalise to data
that was not observed during the process of finding and tuning a model. Usually,
this is done by splitting the available data into three parts: one for training,
one for evaluation during model tuning (such as identifying a reasonable maximum
epoch like here) and, finally, a part that will only be used for validation once
all decisions have been made, to assess the actual generalisation performance of
the final model.
%
Here, we are technically still in the first stage. In the future, potentially,
once a single, promising model has been identified and all decisions have been
made, there must be anothe validation on completely unseen data.

As a first sanity check for the GNN approach, we verify whether the optimisation
procedure is able to overfit the model on the training data. Indeed, the overall
loss value converges to a stable minimum and the AUC score for evaluation on the
training set at the last epoch is $0.989$, indicating a near-perfect fit (not
shown in figures).

% TODO move this somewhere else, does not belong in Discussion
The initialisation of neural network weights before the training depend on
random values. To check whether our training procedure is robust \wrt random
initialisation, we repeat each run several times with different seed values for
the random number generators. Since this seems to be fairly stable, we omit this
in future experiments for sake of simplicity.
% NOTE order of training samples does not matter as we are doing full-batch training

% TODO here, discussion on results
% once we have confirmed that performance is consistent across random seeds
For both evaluation on \PDMap and on \ReconMap, the GNN's ROC curve dominates
the SVM's curve. This means that for a given \FPR deemed acceptable, the GNN
model will provide a higher ratio of True Positives. This advantage grows as the
decision threshold is lowered, i.e. the GNN model is better particularly when we
aim to identify more than just the high-confidence examples. The difference
between ROC curves is only slight on evaluation on \PDMap but strong on
evaluation on \ReconMap. This may be due to a better ability of the GNN model to
generalise, given different characterstics of \ReconMap.
% TODO can we say more about this now that we know that reorg steps break a lot?

% TODO clarify what validation loss is here?
Note how, for both \PDMap and \ReconMap as evaluation datasets, the validation
loss reaches its minimum after just a handful of training epochs. We
additionally ran the same experiment with a decreased learning rate ($0.01
\rightarrow 0.001$). There, we can see a smoother decrease the loss value, but
also note that the overall achieved minimum is worse than in case of a greater
learning rate (see \reffig{svm-repro-lowlr-reconmapolder-loss}). This makes it
very questionable, how robust model training actually is in this case if the
same progress cannot be reliable replicated in a greater number of smaller steps.
\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-reconmapolder-lowlr/results/config-gnn/loss.png}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\linewidth}
    \pic{svm-repro-reconmapolder-lowlr/results/config-gnn/loss-100.png}
  \end{subfigure}
  \caption{Total loss development with decreased learning rate. The plot shows
    the total loss value at each training epoch of the GNN model, evaluated on
    \ReconMap. The right-hand-side plot a focussed view on the same data.}
  \label{fig:svm-repro-lowlr-reconmapolder-loss}
\end{figure}


% TODO compare to nielsen


\section{Importance of Reorganisation Steps}
\paragraph{Motivation} Recall that in case of Alzheimer's Disease Map, we are
given a sequence of reorganisation steps. The original approach of \nielsen was
train a model on the entire sequence of reorganisation steps s.t. the model
would mimic the actions of a human curator as closely as possible. They would
then evaluate the model on \textit{collapsed} versions of other disease maps.
%
A fully collapsed map potentially has vastly different characteristics than a
partially curated map. For instance, a collapsed map is likely to contain
species aliases of very high degree (maybe those that appeared often as
independent aliases before collapse). In a reorganisation step, on the other
hand, the map will already be in part curated and some nodes already duplicated.
%
So, we might effectively be training our models to do one task while testing
them on a slightly different task.
%
This consideration is also relevant if we consider a slightly different
use-case: one of the first steps in the construction of a disease map may be the
semi-automatic concatenation of different pathways, during which species
appearing in several pathways will be mapped to the same species alias, i.e.
there will be exactly one alias per species. Such a map will very closely
resemble a collapsed map in the sense we construct it here.
%
Additionally, a reorganisation sequence is likely to contain contradictory
examples: An alias that is duplicated only in a later step will appear as
negative example in all previous steps. Further, the concatenation of all
reorganisation steps into one big, disconnected graph for training
% TODO verify that we are indeed doing this and describe it earlier
will further amplify the inherent class imbalance.
%
Thus, we deem it interesting to explore how a model will perform if trained not
on the reorganisation steps but simply the collapsed version of the final map.
%
This has the additional advantage that in practise, reorganisation steps will
rarely be available. The collapsed version, however, can be constructed from any
published disease map.

\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.48\linewidth}
    \pic{svm-repro-adlast/results/comparison/roc.png}
    % \pic{svm-repro-adlast/results/}
    % \pic{svm-repro-reconmapolder-lowlr/results/config-gnn/loss.png}
    \caption{(\ADLast $\rightarrow$ \PDMap)}
  \end{subfigure}
  \begin{subfigure}[h]{0.48\linewidth}
    \pic{svm-repro-reconmapolder-adlast/results/comparison/roc.png}
    \caption{(\ADLast $\rightarrow$ \ReconMap)}
  \end{subfigure}
  \caption{ROC curves of models trained on collapsed version of the
    \textit{AlzPathway} map (not the reorganisation steps).}
  \label{fig:importance-reorganisation-steps}
\end{figure}

\paragraph{Results \& Discussion} The results are illustrated in
\reffig{importance-reorganisation-steps}. Note how both SVM and GNN models do
not degrade noticeably in performance (compare to figure \reffig{todo}). In
fact, the SVM model performs \textit{better} when trained on the collapsed map,
potentially due to fewer contradictory examples. The GNN models show similar
performance.

% TODO adress how there is no longer a performance gap between SVM and GNN
% (G)NNs can handle the nasty case better?

Another factor to consider here is that collapsed maps are constructed
programmatically. When dealing with human-created reorganisation steps, it may
be the case that there will be events that may intuitively be interpreted as
duplication events but are not picked correctly up by our inferral of
ground-truth labels. This is another advantage of the approach of considering
only the collapsed version.
% make it sound nicer -- why should our alg not pick it up? due to renaming,
% different ids, ...?
% TODO paragraph needs rewriting and should go somewhere else

Another factor may be that reorganisation steps really are completely
independent, as such considering the steps vs the collapsed map would not
provide any advantages.

% TODO introduce facets of use-case (predict few high-confidence examples AND
% *during curation*) when talking about evaluation of classifiers

% TODO-maybe make it even more clearer that this is a different use-case

% TODO verify this insight (it's been late in the evening)
% TODO if verified, mention this in introduction and discussion in end

% TODO also shows that model is able to learn from 'little' data (719 for prediction).

% TODO somewhere make clear that in this kind of node classification shit like
% undersampling is more complicated to implement since we need to keep the
% entire graph but cut&slice label and label index tensors

% dont *need* reorganisation steps?
% can train on more than one disease map?
% train-on-many

% [[importance of duplication steps vs ...]]


\section{Handling unbalanced classes}
% did this in undersampling-lossweight on GNN model
% grid search over both
% dont have recent plots
% this is all fucked
% possibilities
% - report what we have (no plots, give some tables on grid search) -- means we
% would miss out on possible observation where ROC is pushed to the left
% - try to rerun what we have
% - set up new experiment based on svm-repro -- only need to introduce flags --
% evaluating grid search might suck -- lets just stick to ratios that would make
% the dataset balanced, should be straightforward enough.

% TODO at least *mention* that we did the grid search in undersampling-lossweight

% foootnote \footnote{
%   We omit the evaluation on \ReconMap due to time constraints. In any case, this
%   is still all about finding promising approaches and if it doesnt work for one
%   case, it likely won't help in the other.
% }

% -> svm-repro-undersampling
% -> svm-repro-lossweight


As can be seen from \reffig{maps-summary}, we can expect to deal with unbalanced
data. 
% TODO name the numbers from the figure
% TODO mention that this is particularly relevant for AD reorg *steps*
A possible concern is that the model may become biased towards predicting
the majority class during training since it is simply more likely to occur when
evaluating the overall loss.
% TODO risk

We consider two simple ways to approach this problem. One is to modify the
training data such that it is balanced. Another is to instruct the classifier to
give more importance to examples from the minority class.


% To be sure, that [this helps], it's reasonable to evaluate f1 metrics both for
% the smaller and the larger classes on the validation data. It might show that
% performance on the smaller class becomes better.
% https://datascience.stackexchange.com/a/58739/44723

\paragraph{Undersampling}
The dataset can be made balanced 
% cf undersampling-lossweight
% TODO need to rerun experiment for plots
either by \ild{oversampling} the minory class (coming up with new examples for
the minority class) or by \ild{undersampling} the majority class (dropping
examples from the majority class) of the majority class from the training data.
For oversampling, one may simply duplicate existing examples, or generate
synthetic new examples in a more advanced manner (see
\refsec{sec:related-work}). Although undersampling will decrease the size of the
dataset even further, we stick to this approach for sake of simplicity.
%
We undersample the majority class by considering only a random subset for
prediction. Note that excluded nodes are not removed from the graph (see
\refsec{sec:graph-interpretation}). We undersample the majority class to contain
the same number of examples as the minority class.
% TODO maybe elaborate how exclusion works in a central place, have that in
% preprocessing too.


% TODO svm-repro-adlast-undersampling-sanity absurdly large value makes no difference...?
% but e.g. removing features does impact AUC score (0.4 on only node class)


\paragraph{Weights} 
If a high weight is specified, misclassifications of positive
examples will be penalised more heavily than misclassificaitions of negative examples.
% TODO describe how weights work for SVMs
In case of neural networks, we can introduce an additional coefficient to the
loss function that will determine the weight of prediction outcome of the
positive class. 
%
We consider the \ild{Weighted Binary Cross Entropy} loss as
an extension of \refeq{bce-loss-basic}, given by
\begin{align}
  \label{eq:bce-loss-weighted}
  \mathcal{L}_{\text{BCE weighted}} = \nicefrac{1}{n} \sum_{i=1}^n w_i y_i \log(\pi_i) + (1-y_i) \log (1-\pi_i)
\end{align}
% TODO express with \vec x_i and y_i and parameters to L_BCE
% TODO give proper comparison of BCE *loss* in Background, including
% - mean reduction
% - minimisation instead of maximisation (omitting sign)
We set the weight of the minority class to
$\nicefrac{n_{\text{maj}}}{n_{\text{min}}}$ where $n_{\text{maj}}$ is the number
of examples in the majority
% TODO somewhere define 'examples', majority and minority class
class and $n_{\text{min}}$ the number of examples in the minority class. In
other words, if every example is counted according to its weight, the class
counts are balanced.
% TODO this sounds confusing, maybe put it in formula

\paragraph{Results \& Discussion} Experimental results show that neither
undersampling nor custom class weights have substantial effect on model
performance (\ADLast $\rightarrow$ \PDMap: GNN-AUC$=0.83$; SVM-AUC$=0.83$). We
omit visualisations here due to space constraints.
% TODO maybe upload them to repo and link?
For sake of simplicity, we omit undersampling or weights in future experiments.

% TODO specifying absurdly high value for loss yields AUC of 0.76, this is weird...
% guess i accidentally modified the wrong config file


\section{Importance of Message-Passing}

We explore whether the message-passing layers of the GNN model actually provide
an advantage over a neural network
% TODO consistent capitalisation of 'neural network'
consisting simply of fully-connected layers. Further, we aim to compare which
graph structure serves better for message-passing: We can interpret the given
bipartite graph of species aliases and reactions as a simple graph, or we can
consider its bipartite projection (as in previous experiments).

To assess the importance of using message-passing layers at all, we compare to a
model that has the message-passing layers replaced with fully-connected layers.

Note that for message-passing on the simple graph interpretation, of the
structural features described in TODO, we can only consider those computed on
the simple graph.

\begin{figure}[h]
  \centering
  \begin{subfigure}[h]{0.48\linewidth}
    \pic{svm-repro-adlast-messagepassing/results/comparison/roc.png}
    \caption{ROC Curves for NN models with and without message-passing layers.}
  \end{subfigure}
  \begin{subfigure}[h]{0.48\linewidth}
    \pic{svm-repro-adlast-messagepassing/results/comparison/losses.png}
    \caption{Validation loss per training epoch for different model variants.}
  \end{subfigure}
  \caption{(\ADLast $\rightarrow$ \PDMap) Comparison of performance of NN models
    with and without message-passing layers. \cd{config-gnn-none} has
    fully-connected layers instead of message-passing layers.
    \cd{config-gnn-bipartite} is the same model as considered in previous
    experiments. }
  \label{fig:importance-message-passing}
\end{figure}

Results are summarised in \reffig{fig:importance-message-passing}. It is evident
that on this dataset, using these features
\footnote{
  We will consider a different feature set in TODO.
},
message-passing does not provide a big advantage in overall performance.

However, note that loss curves look different TODO.
% may also be due to that we have more connections now?

% results on MP on simple vs bipartite are not very meaningful since MP itself
% does not make a real difference.
Further, the results on the graph interpretation are probably not meaningful
since the message-passing itself does not make a real difference.

% TODO evaluation on ReconMap?

% without any message-passing
% replace MP-layers with full-connected layers (since GCN model does extraction aswell)
% svm-repro-adlast-messagepassing / config-gnn-none


% TODO somewhere, at least in talk, give something clearer than an ROC curve
% that shows 'so how good are we doing now *really*, practically?'


\section{Attention Mechanism}

% just run and report so we can write it down...
% previously did that on train-on-many so idk

% NOTE no gains on ADLast, similar but slightly worse behaviour altogether...
% TODO maybe see if that helps something when supplying w/ tons of training data (but
% probably not)


\section{Gene Ontology Annotations}

% want to see whether these features 'help'
% i.e. if performance improves if we add them on top of all other features?


% just run constant-embed and constant-stddev and then conclude that information
% is not meaningful, with a big asterisk that the generation of embeddings is
% super lacking


% DONE config-gat-baseline
% DONE config-gat-constant-stddev
% DONE config-gat-degree-stddev
% DONE config-gat-stddev
% TODO config-gcn-baseline
% TODO config-gcn-embed
% TODO config-gcn-stddev

% for SVM
% DONE config-svm-baseline
% DONE config-svm-stddev



% TODO motivate why we only evaluate for PDMap19, why we have no annots for ReconMap


% TODO are the species or the aliases annotated?

% TODO pretty clar tht this is not meaningful, if we have complex of bunch of
% species, each with ~30 GO terms and we average all that junk


% consider only subgraph for which we do hav features
% (but maybe put that in section for experiments)



\section{Feature Selection}
% experiment feature-importance (and others?)
% cf [[feature-importance]]

In their initial publication \cite{nielsen_MachineLearningSupport_2019},
\nielsen used a set of features mainly based on different node centrality
measures, as well as statistics on the centralities of a node's neighbours. They
remark that elaborating the usefulness of different features is left open to
future work. In this section, we aim to explore this direction.

While there are sophisticated approaches to automatic feature selection  
\cite{saeys_ReviewFeatureSelection_2007},
for sake of simplicity we simply train and compare models on different subsets
of the available features.

For each of the structural features defined in \refsec{feature-selection},
\nielsen provide the same characteristic computed both on the simple graph
interpretation and on the graph's bipartite projection. We aim to assess if
indeed both variants are needed or if one alone works better.
% TODO define term 'predictors' somewhere

Further, we check the predictive value of information on node degrees.
This interesting because node degree has been used as a heuristic for
classifying currency metabolites in the context of the analysis of metabolic
models (see \nameref{sec:related-work}). The basic idea is that if a node's
degree is particularly large, it is likely that the connectivity it implies
between its neighbours is not semantically meaningful (i.e. implies \ild{false
  connectivity} as discussed in \refsec{draw-biol-networks}).

Beyond the undirected degree, we additionally consider in- and out-degree. This
is interesting, because directed degrees may have a biological interpretation: A
species that occurs with high out-degree (resp. in-degree) may mainly appear as
substrate or enabling factor (resp. product or result). A node with balanced in-
and out-degree on the other hand may rather represent a central step or even a
key connector.
% TODO related to betweenness
% TODO this is weakened by that we put edges in both directions for side products...


\begin{figure}[h]
  \centering
  \begin{subfigure}{0.48\linewidth}
    \pic{feature-importance/results/comparison/roc-pdmap-bak.png}
    \caption{(\ADLast $\rightarrow$ \PDMap)}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \pic{feature-importance/results/comparison/roc-reconmap-bak.png}
    \caption{(\ADLast $\rightarrow$ \ReconMap)}
  \end{subfigure}
  \caption[Comparison of performance with different feature sets.]{Comparison of
    performance with different feature sets. \cd{config-gnn-basic-both} is the
    feature set considered in earlier experiments and its curve shows the same
    data as in \reffig{importance-reorganisation-steps}.}
  \label{fig:feature-importance}
\end{figure}

% TODO rotate column headers like here  https://tex.stackexchange.com/questions/32683/rotated-column-titles-in-tabular
\begin{table}[h]
  \centering
\begin{tabular}[h]{r | c c c c c}
  & \cd{basic-both} & \cd{basic-projection} & \cd{basic-simple} & \cd{degrees} & \cd{degrees-basic} \\
  \hline
  \cd{betweenness_centrality} & \chk & - & \chk & -  & -  \\
  ...\cd{projection} & \chk & \chk & -  & -  & -  \\
  \cd{closeness_centrality} & \chk & -  & \chk & -  &-  \\
  ...\cd{projection} & \chk & \chk & -  & -  & -  \\
  \cd{eigenvector_centrality} & \chk & - & \chk & -  &- \\
  ...\cd{projection} & \chk & \chk & -  & -  & -  \\
  \cd{neighbour_centrality_statistics} & \chk & - & \chk & -  & - \\
  ...\cd{projection}  & \chk  & \chk & - & -  &- \\
  \cd{distance_set_size}  & \chk & \chk  & \chk & - & - \\
  \cd{clustering_coefficient}  & \chk & \chk & \chk & -  &- \\
  \cd{node_class_onehot} & \chk & \chk & \chk & -  &- \\
  \cd{node_degree} & \chk & - & \chk & \chk & \chk \\
  ...\cd{projection} & \chk & \chk & - & \chk & \chk \\
  \cd{node_in_degree} & - & - & -  & \chk & - \\
  \cd{node_out_degree} & - & -  & -  & \chk & - \\
\end{tabular}
  \caption{Overview over the different feature sets.}
  \label{tab:feature-importance-features}
\end{table}

Results are given in \reffig{feature-importance}. The used feature sets are
elaborated in \reftab{feature-importance-features}. \cd{basic-both} represents the
feature set considered in earlier experiments (similar to the one considered by
\nielsen) and acts as a baseline to compare against.

The results indicate that it makes little difference whether structural features
based on the bipartite projection, the simple graph interpretation or both are
used. Using both variants even seems to slightly hurt model performance. This is
potentially because then the classifier has to deal with input of much higher
dimensionality but little more useful information.
%
It seems it makes little difference which variant is used. The ROC curves for the simple
and the bipartite projection variants almost coincide except for a handful of
predictions.

Most interestingly, we can see that the node degree alone is already a strong
indicator for node duplication. Further, the different behaviour between
evaluation on \PDMap and on \ReconMap is striking. 
For evaluation on \PDMap, using only degrees as
features provides only a relatively small disadvantage. On \ReconMap, however,
using undirected degrees yields a significantly worse performance, while
considering the directed degree actually matches the AUC score of the models
using the full feature sets. This hints at that \PDMap and \ReconMap show
different characteristics in network structure. We discuss this further in (TODO
ref either Discussion or Future Work).
% TODO future work: looking at examples would be interesting here
% TODO future work: much more simple classifier on node degree, or see if not
% simply threshold would do.




\section{Additional Training Data}

...
% seems hard to find other maps that are somewhat comparable...
% TODO mention this in outlook?


\section{Attachment of Edges}
% TODO show some example plots
% TODO can we compare with the actual reattachment as done in the reorganisation
% steps? is this sth like 'enrichment analysis'? or could use comparison scores
% for clusterings like mutual information?

% TODO maybe plot number of duplicates for given disease maps

% TODO make clear that this could be used with other distances measures like on ostaszwewski_clustering

We provide some examples for successful and problematic cases in
\reffig{fig:neighb-clust-examples}. From manual inspection, the vast majority of
results seemed to be reasonable clusterings, and in rare cases, ambiguous
situations like illustrated here occured.

% TODO maybe do comparison to *real* split? (or future work)

% TODO vertical alignment
\begin{figure}[h]
  \centering
  \begin{subfigure}{0.48\linewidth}
    % example where it works nicely
    \includegraphics[width=1.0\textwidth]{dendrograms/sa40.png}
    \caption{
      Example for which the heuristic yields an intuitive clustering. Note that
      outliers do not distort the clustering result and are assigned their own cluster.
    }
    % sa40
  \end{subfigure}
  ~~~
  \begin{subfigure}{0.48\linewidth}
    % example where 2nd derivative is better than 1st (obs)
    % 220
    \includegraphics[width=\textwidth]{dendrograms/sa220.png}
    \caption{Example in which using the second derivative yields a clustering
      that intuitively seems more reasonable. The first derivative has its
      minimum
      % TODO should be maximum?
      at 2 clusters. 
    }
    % or 856
  \end{subfigure}
  \linebreak
      \begin{subfigure}{0.48\linewidth}
        % example where distributions are tricky (obs)
        \includegraphics[width=\textwidth]{dendrograms/sa982.png}
        \caption{
          Distances between the purple points are not much
          different from the distance between the yellow point and the purple
          cluster (measured by \textit{single linkage}). While the heuristic yields
          only two clusters, it may have also been feasible to create many clusters.
          Indeed, the second derivative curve shows another local maximum at 11 clusters.
        }
        % or sa77
      \end{subfigure}
      ~~~
      \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\textwidth]{dendrograms/sa77-variance.png}
        \caption{Another example where large intra-cluster variance is
          problematic. Contrary to the heuristic's output, it may be preferrable
          to further split the yellow cluster.}
      \end{subfigure}
      \caption[
      Example outputs of the heuristic for attaching edges after node
      duplication has been decided.
      ]
      {
    Examples for the clustering procedure described in \refsec{edge-attachment}. The topmost scatterplot shows the positions of neighbours
    of a given node in the layout (given node and edges are not shown). Below, a clustering dendrogram describes
    the distances between two any clusters as they were merged. Finally, the
    sequence of merge distances as well as its first and second derivative.
    Taking the minumum, resp. maximum yields a hint for the number of clusters
    to choose. The red line indicates the suggested choice for the number of
    clusters and thus a concrete clustering, determing the merge node in the
    dendrogram where the ``cut'' should be made.
    % TODO reverse step size, should not be plotted as minimum here.
    All examples are from \ADLast with positive ground-truth label.}
  \label{fig:neighb-clust-examples}
\end{figure}


% ====================================
\chapter{Discussion}

% musings on definition of train/test tasks, internal split, more data

% ambuigity, difficulty
% (make sure we dont mention that too often...)
% - user study in nielsen et all even shows disagreement between experts


% ====================================
\chapter{Outlook \& Future Work}
\label{sec:outlook}

% more training data...


% positoning of duplicates

% using layout information
% - tension
% - stress
% need to take care that computed values are transferrable across maps


%  model learns while user is duplicating?

%  more data
%  - more disease maps
% - other kinds of networks (metabolic models)

% \citeauthor{tiezzi_GraphNeuralNetworks_2021} for node dup?



\pagebreak



\appendix % From here onwards, chapters are numbered with letters, as is the appendix convention

\pagelayout{wide} % No margins
\addpart{Appendix}



\section{Notation \& Abbreviations  }
\begin{itemize}
\item Vectors and matrices are usually set in bold letters, e.g. $\vec x$ or
  $\mat A$. Vectors are assumed to column vectors unless otherwise specified.
  $\cdot^T$ denotes the transpose.
\item $\norm{\cdot}$ denotes the $l_2$-norm, or set cardinality.
\item $G$ commonly denotes a graph, $V(G)$ is its vertex set and $E(G)$ its edge
  set. Whether we consider a directed, undirected, bipartite or simple graph
  will be made clear from context.
\item $\neighb(v)$ denotes the graph neighbourhood of node $v$. Unless otherwise
  specified, this is the direct 1-hop neighbourhood.
\item The \ild{bipartite projection} of a bipartite graph with node set $V = A
  \cupdot B$ that is the disjoint union of species aliases $A$ and reactions
  $B$, the \ild{bipartite projection onto $A$} is the simple graph with node set
  $A$ in which $v_i, v_j \in A$ are connected if and only if in the bipartite
  they have a common neighbour in $B$.
\item A \ild{species} is an actor in a biological system. Examples for species
  are proteins, smaller molecules, but also drugs or phenotypes. A \ild{species
    alias} is a visual representation of a species in a drawing. If disease map
  is interpreted as a graph, a species alias can be thought of as a node in the
  graph, see \refsec{graph-interpretation}. There can be multiple species
  aliases corresponding to the same species. A \ild{complex species alias}
  represents a collection of species aliases, commonly representing protein
  complexes.
\end{itemize}

% TODO vertex splitting alias to node duplication

% TODO use reftex index feature?


\section{Acknowledgements}

Special thanks go to Sune S. Nielsen and Marek Ostaszewski for kindly
answering questions and being available for discussion. I also thank Karsten
Klein, Matthias Rupp and particularly Michael Aichem for providing me with
valuable feedback, guiding me through the process and being great people to work with.
% TODO bit much?



\pagelayout{margin} % Restore margins

%----------------------------------------------------------------------------------------

\backmatter % Denotes the end of the main document content
\setchapterstyle{plain} % Output plain chapters from this point onwards

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% The bibliography needs to be compiled with biber using your LaTeX editor, or on the command line with 'biber main' from the template directory

% \defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
% \printbibliography[heading=bibintoc, title=References, prenote=bibnote] % Add the bibliography heading to the ToC, set the title of the bibliography and output the bibliography note
\printbibliography[heading=bibintoc, title=References] % Add the bibliography heading to the ToC, set the title of the bibliography and output the bibliography note

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

% The index needs to be compiled on the command line with 'makeindex main' from the template directory

\printindex % Output the index

\end{document}
